{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method for option pricing\n",
    "\n",
    "Linus Wunderlich, 2020\n",
    "\n",
    "Method based on https://arxiv.org/abs/1708.07469\n",
    "\n",
    "Parts of the code based on https://github.com/adolfocorreia/DGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_state = 1\n",
    "\n",
    "riskfree_rate = 0.05\n",
    "volatility = 0.25\n",
    "strike_price = 0.5\n",
    "\n",
    "# Time limits\n",
    "T_min = 0.\n",
    "T_max  = 1.\n",
    "\n",
    "# Space limits\n",
    "S_min = 1e-10 \n",
    "S_max = 1.\n",
    "\n",
    "# Network parameters\n",
    "nr_layers = 3\n",
    "nr_nodes_per_layer = 50\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate_decay_steps = 10000\n",
    "learning_rate_decay_rate = 0.9\n",
    "\n",
    "# Training parameters\n",
    "steps_per_sample = 1\n",
    "nr_epochs = 8000\n",
    "\n",
    "# Number of samples\n",
    "N_interior = 1000\n",
    "N_initial = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGM neural network model\n",
    "class DNN(tf.keras.Model):#creating a class called DNN\n",
    "    def __init__(self, nr_layers, nr_nodes_each_layer, state_dimension=1):#init is similiar to a constructor in c++\n",
    "        #self allows you to call instances of that class similar to a this pointer.\n",
    "        tf.keras.Model.__init__(self)#calls the parent constructor\n",
    "        self.nr_layers = nr_layers #assinging member variables nr_layers as input varaible \n",
    "\n",
    "        self.initial_layer = DenseLayer(state_dimension + 1, nr_nodes_each_layer, activation=tf.nn.tanh)\n",
    "        #setting parameters for initial layers start from 2(state dimension +time(1)) nodes all the way to 50 nodes\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(nr_layers): #iterating over the 3 layers\n",
    "            self.hidden_layers.append(LayerFromPaper(state_dimension + 1, nr_nodes_each_layer, activation=tf.nn.tanh))#appending the hidden layers create 3 of them\n",
    "        self.final_layer = DenseLayer(nr_nodes_each_layer, 1, activation=None)#create the last layer\n",
    "\n",
    "\n",
    "    def call(self, t, x):# creating of a member function\n",
    "        X = tf.concat([t,x], 1) # concats the time and stock price in columns\n",
    "\n",
    "        S = self.initial_layer.call(X)#call is a member function of dense layer\n",
    "        for i in range(self.nr_layers):\n",
    "            S = self.hidden_layers[i].call({'S': S, 'X': X})#creating the hidden layers, #X=time and asset price we concat this in  X = tf.concat([t,x], 1)\n",
    "        result = self.final_layer.call(S)#creating the final layer\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "# Neural network layers\n",
    "\n",
    "class DenseLayer(tf.keras.layers.Layer):# creating the class Dense layers\n",
    "    def __init__(self, nr_inputs, nr_outputs, activation): #creating the constructor for that class\n",
    "        tf.keras.layers.Layer.__init__(self)#initialzing the object of that class\n",
    "        \n",
    "        self.initializer = tf.keras.initializers.glorot_normal\n",
    "        #self.initializer=tf.contrib.layers.xavier_initializer()) #TF 1\n",
    "\n",
    "        self.nr_inputs = nr_inputs# initilaizing nr_inputs as a member variable\n",
    "        self.nr_outputs = nr_outputs # initilizing nr_outputs as a member varaible\n",
    "        \n",
    "        self.W = self.add_variable(\"W\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                   initializer=self.initializer())# initializing W as a member variable creating a matrix type object in order to train it for the neural network\n",
    "        #W is one of the weights\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.nr_outputs])  #bias or constant added only at the end of training as therefore has no initilization\n",
    "\n",
    "        self.activation = activation #saving it as a member variable\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):#member function of Dense Layer\n",
    "        S = tf.add(tf.matmul(inputs, self.W), self.b) #From paper\n",
    "        if not self.activation == None:\n",
    "            S = self.activation(S) #activation function with the sigma (not volatility)\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "\n",
    "class LayerFromPaper(tf.keras.layers.Layer):\n",
    "    def __init__(self, nr_inputs, nr_outputs, activation):\n",
    "        tf.keras.layers.Layer.__init__(self)\n",
    "\n",
    "        self.initializer = tf.keras.initializers.glorot_normal\n",
    "        #self.initializer=tf.contrib.layers.xavier_initializer()) #TF 1\n",
    "        \n",
    "        self.nr_outputs = nr_outputs\n",
    "        self.nr_inputs = nr_inputs\n",
    "\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.nr_outputs])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.nr_outputs])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.nr_outputs])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.nr_outputs])\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        S = inputs['S']\n",
    "        X = inputs['X']\n",
    "\n",
    "        Z = self.activation(tf.add(tf.add(tf.matmul(X, self.Uz), tf.matmul(S, self.Wz)), self.bz))\n",
    "        G = self.activation(tf.add(tf.add(tf.matmul(X, self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.activation(tf.add(tf.add(tf.matmul(X, self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        H = self.activation(tf.add(tf.add(tf.matmul(X, self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        Snew = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z, S))\n",
    "\n",
    "        return Snew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def get_residual(model, t_interior, x_interior, t_initial, x_initial):#model =BS Model,t_interior=time from 0 to 1,\n",
    "    #x_interior=stock price from min to max, t_initial =t_min, x_initial=S_min\n",
    "    # Loss term #1: PDE\n",
    "    V = model(t_interior, x_interior)# valuation of the neural network\n",
    "    V_t = tf.gradients(V, t_interior)[0] # DV/DT\n",
    "    V_x = tf.gradients(V, x_interior)[0] # DV/DS\n",
    "    V_xx = tf.gradients(V_x, x_interior)[0] #D^2V/DS^2\n",
    "    #black scholes formula\n",
    "    f = -V_t + riskfree_rate * V -riskfree_rate*x_interior*V_x - 0.5*volatility**2 * x_interior**2 * V_xx\n",
    "\n",
    "    L1 = tf.reduce_mean(tf.square(f)) #mean of the squared residuals, residuals of the PDE J(F) part 1\n",
    "    #Payoff function\n",
    "    payoff= tf.math.maximum(0.,tf.subtract(x_interior,strike_price))\n",
    "    #L2 norm\n",
    "    L2=tf.reduce_mean(tf.square(tf.math.maximum(0.,payoff-V)))#J(f) part 2\n",
    "    #max deviation of L2\n",
    "    Max_dev=tf.math.reduce_max(tf.math.maximum(0.,V-payoff))\n",
    "    #min deviation of L2\n",
    "    Min_dev=tf.math.reduce_min(tf.math.maximum(0.,V-payoff))\n",
    "    #Multiplying L2 with a constant\n",
    "    L2_multiplier=tf.math.multiply(L2,10)\n",
    "    # Loss term #3: initial/terminal condition\n",
    "    L3 = tf.reduce_mean(tf.square(model(t_initial,x_initial) - tf.math.maximum(0., x_initial - strike_price))) # J(F) part 3\n",
    "\n",
    "    return (Max_dev,L1,L2,L2_multiplier,L3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Sampling\n",
    "def get_monte_carlo_points(N_interior, N_initial):\n",
    "    # Sampler #1: PDE domain\n",
    "    t_interior = np.random.uniform(low=T_min - 0.5*(T_max - T_min),\n",
    "                           high=T_max,\n",
    "                           size=[N_interior,1])\n",
    "    s_interior = np.random.uniform(low=S_min - (S_max - S_min)*0.5,\n",
    "                           high=S_max + (S_max - S_min)*0.5,\n",
    "                           size=[N_interior,1])\n",
    "    #you take all the t and the state space\n",
    "    \n",
    "    # Sampler #2: initial/terminal condition\n",
    "    t_initial = T_max * np.ones((N_initial,1)) #Terminal condition\n",
    "    s_initial = np.random.uniform(low=S_min - (S_max - S_min)*0.5,\n",
    "                           high=S_max + (S_max - S_min)*0.5,\n",
    "                           size=[N_initial,1])\n",
    "    \n",
    "    return (t_interior, s_interior, t_initial, s_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "This is \n",
      "Tensor(\"Mean_19:0\", shape=(), dtype=float32)\n",
      "This is \n",
      "Tensor(\"Mean_18:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x1a3b4d5940>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "# Neural Network definition\n",
    "\n",
    "model = DNN(nr_layers, nr_nodes_per_layer) #first time this model is constructed\n",
    "\n",
    "t_interior_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"time_interior\") #allows you to fill it with numbers\n",
    "x_interior_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, dimension_state), name=\"stock_prices_interior\")\n",
    "t_initial_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"time_initial\")#allows you to fill it with numbers\n",
    "x_initial_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, dimension_state), name=\"stock_prices_initial\")\n",
    "\n",
    "\n",
    "Max_dev,residual_interior,residual_exterior,L2_multiplier,residual_initial= get_residual(model, t_interior_tf, x_interior_tf, t_initial_tf, x_initial_tf)\n",
    "print(\"This is \")\n",
    "print(residual_exterior)\n",
    "print(\"This is \")\n",
    "print(residual_interior)\n",
    "residual = residual_interior + residual_initial+residual_exterior #this residual is the combination of the L1 and L2 norm\n",
    "# Optimizer parameters\n",
    "nr_steps = tf.Variable(0, trainable=False) #very weird itertation counter, counts the no of optimization steps we took\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(initial_learning_rate, nr_steps,\n",
    "                                           learning_rate_decay_steps, \n",
    "                                           learning_rate_decay_rate, staircase=True)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(residual) #defining the optimizer \n",
    "# gradient descent with a momemtum term. mizimised the residuals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot tensors\n",
    "tplot_t = tf.compat.v1.placeholder(tf.float32, [None,1], name=\"tplot_t\") # We name to recover it later\n",
    "xplot_t = tf.compat.v1.placeholder(tf.float32, [None,1], name=\"xplot_t\")\n",
    "vplot_t = tf.identity(model(tplot_t, xplot_t), name=\"vplot_t\") # Trick for naming the trained model\n",
    "\n",
    "\n",
    "# Training data holders\n",
    "residuals_list = []\n",
    "\n",
    "# Train network!!\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 0000, Loss: 1.676183e+00, Maximum_Deviation: 2.391926e+00, L1: 8.265582e-02, L2: 0.000000e+00,L2_mutiplier: 0.000000e+00 L3: 2.309270e+00\n"
     ]
    }
   ],
   "source": [
    "# before opening a tensorflow session, close the old one if possible\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass \n",
    "sess =  tf.compat.v1.Session()\n",
    "\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    t_interior_mc, x_interior_mc, t_initial_mc, x_initial_mc = get_monte_carlo_points(N_interior, N_initial)\n",
    "\n",
    "    for _ in range(steps_per_sample):\n",
    "        Max_deviation,residual_value, residual_interior_value,residual_exterior_value,L2_multiplier_value,residual_initial_value, _ = \\\n",
    "            sess.run([Max_dev,residual, residual_interior, residual_exterior,L2_multiplier,residual_initial, optimizer],\n",
    "                  feed_dict = {t_interior_tf:t_interior_mc, x_interior_tf:x_interior_mc,\n",
    "                                t_initial_tf:t_initial_mc, x_initial_tf:x_initial_mc})\n",
    "            \n",
    "\n",
    "    residuals_list.append(residual_value)\n",
    "\n",
    "    if (not np.mod(epoch, 100)) or epoch+1==nr_epochs:\n",
    "        print(\"Stage: {:04d}, Loss: {:e}, Maximum_Deviation: {:e}, L1: {:e}, L2: {:e},L2_mutiplier: {:e} L3: {:e}\".format(\n",
    "            epoch,Max_deviation, residual_value, residual_interior_value,residual_exterior_value,L2_multiplier_value,residual_initial_value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot results\n",
    "N = 41      # Points on plot grid\n",
    "\n",
    "times_to_plot = [0*T_max, 0.33*T_max, 0.66*T_max, T_max]\n",
    "tplot = np.linspace(T_min, T_max, N) # for surface plot\n",
    "xplot = np.linspace(S_min, S_max, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "i = 1\n",
    "for t in times_to_plot:\n",
    "    tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "    nn_plot, = sess.run([vplot_t],\n",
    "                        feed_dict={tplot_t:tt, xplot_t:xplot.reshape(-1,1)})\n",
    "\n",
    "    plt.subplot(2,2,i)\n",
    "    plt.plot(xplot, nn_plot, 'r')\n",
    "\n",
    "    plt.ylim(-1.1, 1.2)\n",
    "    plt.xlabel(\"S\")\n",
    "    plt.ylabel(\"V\")\n",
    "    plt.title(\"t = %.2f\"%t, loc=\"left\")\n",
    "    i = i+1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_dimensional_bs_solution(time, state, strike_price, volatility, riskfree_rate):\n",
    "    drift = riskfree_rate\n",
    "    if np.size(volatility) == 1:  # scalar sigma\n",
    "        volatility = volatility * np.ones(np.shape(time))\n",
    "    if np.size(strike_price) == 1:  # scalar sigma\n",
    "        strike_price = strike_price * np.ones(np.shape(time))\n",
    "    if np.size(drift) == 1:  # scalar sigma\n",
    "        drift = drift * np.ones(np.shape(time))\n",
    "    if np.size(riskfree_rate) == 1:  # scalar sigma\n",
    "        riskfree_rate = riskfree_rate * np.ones(np.shape(time))\n",
    "\n",
    "    solution = np.zeros(np.shape(time))\n",
    "    d1 = np.zeros(np.shape(time))\n",
    "    d2 = np.zeros(np.shape(time))\n",
    "\n",
    "    is_initial_time = (time == 0)\n",
    "    is_zero = (state == 0)\n",
    "    is_not_special_case = (time > 0) & (state > 0)\n",
    "\n",
    "    d1[is_not_special_case] = 1. / (volatility[is_not_special_case] * np.sqrt(time[is_not_special_case])) * (\n",
    "            np.log(state[is_not_special_case] / strike_price[is_not_special_case]) + (\n",
    "            drift[is_not_special_case] + volatility[is_not_special_case] ** 2 * 0.5) * time[is_not_special_case])\n",
    "    d2[is_not_special_case] = d1[is_not_special_case] \\\n",
    "                              - volatility[is_not_special_case] * np.sqrt(time[is_not_special_case])\n",
    "\n",
    "    solution[is_not_special_case] = state[is_not_special_case] * norm.cdf(d1[is_not_special_case]) - strike_price[\n",
    "        is_not_special_case] * np.exp(-riskfree_rate[is_not_special_case] * time[is_not_special_case]) * norm.cdf(\n",
    "        d2[is_not_special_case])\n",
    "\n",
    "    solution[is_initial_time] = np.maximum(0, state[is_initial_time] - strike_price[is_initial_time])\n",
    "    solution[is_zero] = 0.\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "i = 1\n",
    "for t in times_to_plot:\n",
    "    tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "    nn_plot, = sess.run([vplot_t],\n",
    "                        feed_dict={tplot_t:tt, xplot_t:xplot.reshape(-1,1)})\n",
    "\n",
    "    exact_plot = one_dimensional_bs_solution(\n",
    "        T_max-tt, xplot.reshape(-1,1), strike_price, volatility, riskfree_rate)\n",
    "    \n",
    "    plt.subplot(2,2,i)\n",
    "    plt.plot(xplot, nn_plot, 'r')\n",
    "    plt.plot(xplot, exact_plot, 'b')\n",
    "\n",
    "    plt.ylim(-1.1, 1.2)\n",
    "    plt.xlabel(\"S\")\n",
    "    plt.ylabel(\"V\")\n",
    "    plt.title(\"t = %.2f\"%t, loc=\"left\")\n",
    "    i = i+1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "i = 1\n",
    "for t in times_to_plot:\n",
    "    tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "    nn_plot, = sess.run([vplot_t],\n",
    "                        feed_dict={tplot_t:tt, xplot_t:xplot.reshape(-1,1)})\n",
    "\n",
    "    exact_plot = one_dimensional_bs_solution(\n",
    "        T_max-tt, xplot.reshape(-1,1), strike_price, volatility, riskfree_rate)\n",
    "    \n",
    "    plt.subplot(2,2,i)\n",
    "    plt.plot(xplot, nn_plot-exact_plot, 'r')\n",
    "    plt.plot(xplot, exact_plot, 'b-')\n",
    "    \n",
    "    plt.ylim(-0.004, 0.004)\n",
    "\n",
    "    plt.xlabel(\"S\")\n",
    "    plt.ylabel(\"Absolute Error\")\n",
    "    plt.title(\"t = %.2f\"%t, loc=\"left\")\n",
    "    i = i+1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "i = 1\n",
    "for t in times_to_plot:\n",
    "    tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "    nn_plot, = sess.run([vplot_t],\n",
    "                        feed_dict={tplot_t:tt, xplot_t:xplot.reshape(-1,1)})\n",
    "\n",
    "    exact_plot = one_dimensional_bs_solution(\n",
    "        T_max-tt, xplot.reshape(-1,1), strike_price, volatility, riskfree_rate)\n",
    "    \n",
    "    plt.subplot(2,2,i)\n",
    "    plt.plot(xplot, (nn_plot-exact_plot)/exact_plot, 'r')\n",
    "\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    plt.xlim(0.4, 0.6)\n",
    "    plt.xlabel(\"S\")\n",
    "    plt.ylabel(\"Relative Error\")\n",
    "    plt.title(\"t = %.2f\"%t, loc=\"left\")\n",
    "    i = i+1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
