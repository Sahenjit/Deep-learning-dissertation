{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyPa4iI7jUeJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib import cm\n",
    "from scipy.stats import norm\n",
    "import pylab\n",
    "import pandas as pd\n",
    "import scipy.interpolate as interp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UW5rlBIjUen"
   },
   "outputs": [],
   "source": [
    "dimension_state = 1\n",
    "\n",
    "riskfree_rate = 0.05\n",
    "volatility = 0.25\n",
    "strike_price = 0.5\n",
    "\n",
    "# Time limits\n",
    "T_min = 0.\n",
    "T_max  = 1.\n",
    "\n",
    "# Space limits\n",
    "S_min = 1e-10 \n",
    "S_max = 1.\n",
    "\n",
    "# Network parameters\n",
    "nr_layers = 3\n",
    "nr_nodes_per_layer = 50\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate_decay_steps = 10000\n",
    "learning_rate_decay_rate = 0.9\n",
    "\n",
    "# Training parameters\n",
    "steps_per_sample = 10\n",
    "nr_epochs = 1000\n",
    "\n",
    "# Number of samples\n",
    "N_interior = 10000\n",
    "N_initial = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RuL5PdFjUe7"
   },
   "outputs": [],
   "source": [
    "# DGM neural network model\n",
    "class DNN(tf.keras.Model):#creating a class called DNN\n",
    "    def __init__(self, nr_layers, nr_nodes_each_layer, state_dimension=1):#init is similiar to a constructor in c++\n",
    "        #self allows you to call instances of that class similar to a this pointer.\n",
    "        tf.keras.Model.__init__(self)#calls the parent constructor\n",
    "        self.nr_layers = nr_layers #assinging member variables nr_layers as input varaible \n",
    "\n",
    "        self.initial_layer = DenseLayer(state_dimension + 1, nr_nodes_each_layer, activation=tf.nn.tanh)\n",
    "        #setting parameters for initial layers start from 2(state dimension +time(1)) nodes all the way to 50 nodes\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(nr_layers): #iterating over the 3 layers\n",
    "            self.hidden_layers.append(LayerFromPaper(state_dimension + 1, nr_nodes_each_layer, activation=tf.nn.tanh))#appending the hidden layers create 3 of them\n",
    "        self.final_layer = DenseLayer(nr_nodes_each_layer, 1, activation=None)#create the last layer\n",
    "\n",
    "\n",
    "    def call(self, t, x):# creating of a member function\n",
    "        X = tf.concat([t,x], 1) # concats the time and stock price in columns\n",
    "\n",
    "        S = self.initial_layer.call(X)#call is a member function of dense layer\n",
    "        for i in range(self.nr_layers):\n",
    "            S = self.hidden_layers[i].call({'S': S, 'X': X})#creating the hidden layers, #X=time and asset price we concat this in  X = tf.concat([t,x], 1)\n",
    "        result = self.final_layer.call(S)#creating the final layer\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "# Neural network layers\n",
    "\n",
    "class DenseLayer(tf.keras.layers.Layer):# creating the class Dense layers\n",
    "    def __init__(self, nr_inputs, nr_outputs, activation): #creating the constructor for that class\n",
    "        tf.keras.layers.Layer.__init__(self)#initialzing the object of that class\n",
    "        \n",
    "        self.initializer = tf.keras.initializers.glorot_normal\n",
    "        #self.initializer=tf.contrib.layers.xavier_initializer()) #TF 1\n",
    "\n",
    "        self.nr_inputs = nr_inputs# initilaizing nr_inputs as a member variable\n",
    "        self.nr_outputs = nr_outputs # initilizing nr_outputs as a member varaible\n",
    "        \n",
    "        self.W = self.add_variable(\"W\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                   initializer=self.initializer())# initializing W as a member variable creating a matrix type object in order to train it for the neural network\n",
    "        #W is one of the weights\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.nr_outputs])  #bias or constant added only at the end of training as therefore has no initilization\n",
    "\n",
    "        self.activation = activation #saving it as a member variable\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):#member function of Dense Layer\n",
    "        S = tf.add(tf.matmul(inputs, self.W), self.b) #From paper\n",
    "        if not self.activation == None:\n",
    "            S = self.activation(S) #activation function with the sigma (not volatility)\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "\n",
    "class LayerFromPaper(tf.keras.layers.Layer):\n",
    "    def __init__(self, nr_inputs, nr_outputs, activation):\n",
    "        tf.keras.layers.Layer.__init__(self)\n",
    "\n",
    "        self.initializer = tf.keras.initializers.glorot_normal\n",
    "        \n",
    "        self.nr_outputs = nr_outputs\n",
    "        self.nr_inputs = nr_inputs\n",
    "\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.nr_outputs])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.nr_outputs])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.nr_outputs])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.nr_outputs])\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        S = inputs['S']\n",
    "        X = inputs['X']\n",
    "\n",
    "        Z = self.activation(tf.add(tf.add(tf.matmul(X, self.Uz), tf.matmul(S, self.Wz)), self.bz))\n",
    "        G = self.activation(tf.add(tf.add(tf.matmul(X, self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.activation(tf.add(tf.add(tf.matmul(X, self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        H = self.activation(tf.add(tf.add(tf.matmul(X, self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        Snew = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z, S))\n",
    "\n",
    "        return Snew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cz51HONIjUfJ"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def get_residual(model, t_interior, x_interior, t_initial, x_initial):#model =BS Model,t_interior=time from 0 to 1,\n",
    "    #x_interior=stock price from min to max, t_initial =t_min, x_initial=S_min\n",
    "    # Loss term #1: PDE\n",
    "    V = model(t_interior, x_interior)# valuation of the neural network\n",
    "    V_t = tf.gradients(V, t_interior)[0] # DV/DT\n",
    "    V_x = tf.gradients(V, x_interior)[0] # DV/DS\n",
    "    V_xx = tf.gradients(V_x, x_interior)[0] #D^2V/DS^2\n",
    "    #black scholes formula\n",
    "    f = -V_t + riskfree_rate * V -riskfree_rate*x_interior*V_x - 0.5*volatility**2 * x_interior**2 * V_xx\n",
    "\n",
    "    L1 = tf.reduce_mean(tf.square(f)) #mean of the squared residuals, residuals of the PDE J(F) part 1\n",
    "    #Payoff function\n",
    "    payoff= tf.math.maximum(0.,tf.subtract(strike_price,x_interior))\n",
    "    #L2 norm\n",
    "    L2=tf.reduce_mean(tf.square(tf.math.maximum(0.,payoff-V)))#J(f) part 2\n",
    "    #max deviation of L2\n",
    "    Max_dev=tf.math.reduce_max(tf.math.maximum(0.,payoff-V))\n",
    "    #min deviation of L2\n",
    "    Min_dev=tf.math.reduce_min(tf.math.maximum(0.,payoff-V))\n",
    "    # Loss term #3: initial/terminal condition\n",
    "    L3 = tf.reduce_mean(tf.square(model(t_initial,x_initial) - tf.math.maximum(0., strike_price - x_initial))) # J(F) part 3\n",
    "\n",
    "    return (Max_dev,L1,L2,L3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWfaf8Y5jUfR"
   },
   "outputs": [],
   "source": [
    "#  Sampling\n",
    "def get_monte_carlo_points(N_interior, N_initial):\n",
    "    # Sampler #1: PDE domain\n",
    "    t_interior = np.random.uniform(low=T_min - 0.5*(T_max - T_min),\n",
    "                           high=T_max,\n",
    "                           size=[N_interior,1])\n",
    "    s_interior = np.random.uniform(low=S_min - (S_max - S_min)*0.5,\n",
    "                           high=S_max + (S_max - S_min)*0.5,\n",
    "                           size=[N_interior,1])\n",
    "    #you take all the t and the state space\n",
    "    \n",
    "    # Sampler #2: initial/terminal condition\n",
    "    t_initial = T_max * np.ones((N_initial,1)) #Terminal condition\n",
    "    s_initial = np.random.uniform(low=S_min - (S_max - S_min)*0.5,\n",
    "                           high=S_max + (S_max - S_min)*0.5,\n",
    "                           size=[N_initial,1])\n",
    "    \n",
    "    return (t_interior, s_interior, t_initial, s_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKSO5UtTjUfe"
   },
   "outputs": [],
   "source": [
    "nn_plot_list=[]\n",
    "def compute_errors():\n",
    "    max_error=0\n",
    "    times_to_evaluate_error=np.linspace(T_min,T_max,10)\n",
    "    for t in times_to_evaluate_error:\n",
    "        tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "    \n",
    "        nn_plot, = sess.run([vplot_t],\n",
    "                        feed_dict={tplot_t:tt, xplot_t:xplot.reshape(-1,1)})\n",
    "        nn_plot_list.append(nn_plot,)\n",
    "    \n",
    "    return nn_plot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zpwkh7a5jUfp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7fdeb6a1ce80>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "# Neural Network definition\n",
    "\n",
    "model = DNN(nr_layers, nr_nodes_per_layer) #first time this model is constructed\n",
    "\n",
    "t_interior_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"time_interior\") #allows you to fill it with numbers\n",
    "x_interior_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, dimension_state), name=\"stock_prices_interior\")\n",
    "t_initial_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"time_initial\")#allows you to fill it with numbers\n",
    "x_initial_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, dimension_state), name=\"stock_prices_initial\")\n",
    "\n",
    "\n",
    "Max_dev,residual_interior,residual_exterior,residual_initial= get_residual(model, t_interior_tf, x_interior_tf, t_initial_tf, x_initial_tf)\n",
    "residual = residual_interior + residual_initial+(residual_exterior*1) #this residual is the combination of the L1 and L2 norm\n",
    "# Optimizer parameters\n",
    "nr_steps = tf.Variable(0, trainable=False) #very weird itertation counter, counts the no of optimization steps we took\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(initial_learning_rate, nr_steps,\n",
    "                                           learning_rate_decay_steps, \n",
    "                                           learning_rate_decay_rate, staircase=True)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(residual) #defining the optimizer \n",
    "# gradient descent with a momemtum term. mizimised the residuals\n",
    "\n",
    "# Plot tensors\n",
    "tplot_t = tf.compat.v1.placeholder(tf.float32, [None,1], name=\"tplot_t\") # We name to recover it later\n",
    "xplot_t = tf.compat.v1.placeholder(tf.float32, [None,1], name=\"xplot_t\")\n",
    "vplot_t = tf.identity(model(tplot_t, xplot_t), name=\"vplot_t\") # Trick for naming the trained model\n",
    "\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "n8oXLkHpjUfz",
    "outputId": "1b2d6583-f410-47e1-8278-da6aac215c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 0000, Loss: 3.351902e-01, Maximum_Deviation: 8.994368e-01, L1: 1.545323e-01, L2: 3.558923e-02, L3: 1.450686e-01\n",
      "Stage: 0100, Loss: 4.011974e-04, Maximum_Deviation: 4.318166e-02, L1: 6.813802e-05, L2: 8.615850e-05, L3: 2.469008e-04\n",
      "Stage: 0200, Loss: 1.980041e-04, Maximum_Deviation: 2.994069e-02, L1: 4.174039e-05, L2: 4.840116e-05, L3: 1.078625e-04\n",
      "Stage: 0300, Loss: 1.479570e-04, Maximum_Deviation: 2.466556e-02, L1: 3.075591e-05, L2: 4.076782e-05, L3: 7.643325e-05\n",
      "Stage: 0400, Loss: 1.261060e-04, Maximum_Deviation: 2.434850e-02, L1: 2.469985e-05, L2: 4.056476e-05, L3: 6.084137e-05\n",
      "Stage: 0500, Loss: 1.072655e-04, Maximum_Deviation: 2.281111e-02, L1: 2.708731e-05, L2: 3.835243e-05, L3: 4.182577e-05\n",
      "Stage: 0600, Loss: 9.721018e-05, Maximum_Deviation: 2.256328e-02, L1: 2.603231e-05, L2: 3.778138e-05, L3: 3.339649e-05\n",
      "Stage: 0700, Loss: 8.543250e-05, Maximum_Deviation: 2.202976e-02, L1: 2.544727e-05, L2: 3.595691e-05, L3: 2.402832e-05\n",
      "Stage: 0800, Loss: 7.901820e-05, Maximum_Deviation: 2.162027e-02, L1: 2.503242e-05, L2: 3.344560e-05, L3: 2.054018e-05\n",
      "Stage: 0900, Loss: 7.803773e-05, Maximum_Deviation: 2.125937e-02, L1: 2.351844e-05, L2: 3.346047e-05, L3: 2.105882e-05\n",
      "Stage: 0999, Loss: 7.486352e-05, Maximum_Deviation: 2.097923e-02, L1: 2.334905e-05, L2: 3.232953e-05, L3: 1.918495e-05\n"
     ]
    }
   ],
   "source": [
    "# before opening a tensorflow session, close the old one if possible\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass \n",
    "sess =  tf.compat.v1.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    t_interior_mc, x_interior_mc, t_initial_mc, x_initial_mc = get_monte_carlo_points(N_interior, N_initial)\n",
    "\n",
    "    for _ in range(steps_per_sample):\n",
    "          Max_deviation,residual_value, residual_interior_value,residual_exterior_value,residual_initial_value, _ = \\\n",
    "              sess.run([Max_dev,residual, residual_interior, residual_exterior,residual_initial, optimizer],\n",
    "                      feed_dict = {t_interior_tf:t_interior_mc, x_interior_tf:x_interior_mc,\n",
    "                                    t_initial_tf:t_initial_mc, x_initial_tf:x_initial_mc})\n",
    "        \n",
    "    if (not np.mod(epoch, 100)) or epoch+1==nr_epochs:\n",
    "        print(\"Stage: {:04d}, Loss: {:e}, Maximum_Deviation: {:e}, L1: {:e}, L2: {:e}, L3: {:e}\".format(\n",
    "            epoch,residual_value,Max_deviation, residual_interior_value,residual_exterior_value,residual_initial_value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_W8UNwQjUf7"
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "N = 41      # Points on plot grid\n",
    "\n",
    "times_to_plot = [0*T_max, 0.33*T_max, 0.66*T_max, T_max]\n",
    "tplot = np.linspace(T_min, T_max, N) # for surface plot\n",
    "xplot = np.linspace(S_min, S_max, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "VB8caAlTjUgE",
    "outputId": "31b3b207-b91b-41d8-b492-f3e66b31000e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         T=0  T=0.1111  T=0.2222  T=0.3333  T=0.4444  T=0.5555  T=0.6666  \\\n",
      "0   0.489975  0.491675  0.493298  0.494863  0.496391  0.497907  0.499440   \n",
      "1   0.464882  0.466580  0.468199  0.469760  0.471283  0.472798  0.474334   \n",
      "2   0.439896  0.441617  0.443256  0.444834  0.446374  0.447904  0.449459   \n",
      "3   0.415001  0.416769  0.418452  0.420069  0.421646  0.423212  0.424798   \n",
      "4   0.390162  0.391997  0.393745  0.395424  0.397058  0.398675  0.400308   \n",
      "5   0.365328  0.367244  0.369073  0.370831  0.372540  0.374225  0.375918   \n",
      "6   0.340437  0.342441  0.344359  0.346208  0.348004  0.349771  0.351536   \n",
      "7   0.315435  0.317517  0.319522  0.321462  0.323352  0.325209  0.327053   \n",
      "8   0.290280  0.292417  0.294489  0.296509  0.298486  0.300431  0.302356   \n",
      "9   0.264969  0.267115  0.269216  0.271284  0.273324  0.275343  0.277342   \n",
      "10  0.239548  0.241638  0.243707  0.245766  0.247823  0.249879  0.251927   \n",
      "11  0.214143  0.216088  0.218037  0.220006  0.222005  0.224034  0.226081   \n",
      "12  0.188969  0.190664  0.192385  0.194152  0.195985  0.197888  0.199850   \n",
      "13  0.164340  0.165676  0.167046  0.168479  0.170007  0.171648  0.173398   \n",
      "14  0.140661  0.141537  0.142438  0.143399  0.144467  0.145677  0.147044   \n",
      "15  0.118387  0.118735  0.119074  0.119446  0.119905  0.120508  0.121294   \n",
      "16  0.097966  0.097764  0.097503  0.097219  0.096968  0.096818  0.096831   \n",
      "17  0.079760  0.079045  0.078212  0.077283  0.076304  0.075340  0.074457   \n",
      "18  0.063984  0.062844  0.061533  0.060054  0.058434  0.056715  0.054945   \n",
      "19  0.050673  0.049231  0.047581  0.045712  0.043627  0.041338  0.038853   \n",
      "20  0.039699  0.038085  0.036249  0.034174  0.031848  0.029254  0.026357   \n",
      "21  0.030819  0.029147  0.027265  0.025154  0.022800  0.020176  0.017216   \n",
      "22  0.023733  0.022094  0.020271  0.018254  0.016036  0.013598  0.010873   \n",
      "23  0.018141  0.016595  0.014902  0.013060  0.011077  0.008954  0.006647   \n",
      "24  0.013766  0.012352  0.010828  0.009200  0.007494  0.005731  0.003907   \n",
      "25  0.010375  0.009112  0.007773  0.006372  0.004944  0.003532  0.002164   \n",
      "26  0.007774  0.006667  0.005516  0.004336  0.003167  0.002064  0.001083   \n",
      "27  0.005802  0.004850  0.003879  0.002905  0.001967  0.001124  0.000445   \n",
      "28  0.004328  0.003521  0.002717  0.001930  0.001194  0.000563  0.000108   \n",
      "29  0.003242  0.002566  0.001913  0.001291  0.000728  0.000267 -0.000027   \n",
      "30  0.002455  0.001891  0.001367  0.000888  0.000470  0.000145 -0.000035   \n",
      "31  0.001893  0.001421  0.001005  0.000644  0.000345  0.000126  0.000023   \n",
      "32  0.001503  0.001099  0.000766  0.000497  0.000292  0.000153  0.000101   \n",
      "33  0.001245  0.000885  0.000609  0.000407  0.000269  0.000188  0.000167   \n",
      "34  0.001094  0.000754  0.000509  0.000347  0.000253  0.000208  0.000200   \n",
      "35  0.001036  0.000692  0.000453  0.000306  0.000232  0.000203  0.000196   \n",
      "36  0.001065  0.000695  0.000438  0.000283  0.000206  0.000176  0.000159   \n",
      "37  0.001185  0.000767  0.000468  0.000281  0.000181  0.000135  0.000100   \n",
      "38  0.001399  0.000913  0.000553  0.000313  0.000171  0.000092  0.000033   \n",
      "39  0.001717  0.001144  0.000701  0.000388  0.000187  0.000063 -0.000027   \n",
      "40  0.002146  0.001469  0.000926  0.000521  0.000242  0.000059 -0.000067   \n",
      "\n",
      "    T=0.7777  T=0.8888       T=1  \n",
      "0   0.501020  0.502686  0.504490  \n",
      "1   0.475927  0.477619  0.479470  \n",
      "2   0.451074  0.452798  0.454699  \n",
      "3   0.426446  0.428207  0.430154  \n",
      "4   0.401998  0.403797  0.405787  \n",
      "5   0.377657  0.379498  0.381523  \n",
      "6   0.353332  0.355214  0.357269  \n",
      "7   0.328912  0.330837  0.332915  \n",
      "8   0.304282  0.306249  0.308345  \n",
      "9   0.279329  0.281337  0.283444  \n",
      "10  0.253961  0.255999  0.258108  \n",
      "11  0.228127  0.230172  0.232266  \n",
      "12  0.201842  0.203847  0.205894  \n",
      "13  0.175228  0.177108  0.179044  \n",
      "14  0.148553  0.150170  0.151887  \n",
      "15  0.122272  0.123428  0.124750  \n",
      "16  0.097050  0.097493  0.098173  \n",
      "17  0.073723  0.073195  0.072934  \n",
      "18  0.053186  0.051512  0.050030  \n",
      "19  0.036186  0.033381  0.030545  \n",
      "20  0.023100  0.019436  0.015394  \n",
      "21  0.013800  0.009769  0.004996  \n",
      "22  0.007712  0.003873 -0.000938  \n",
      "23  0.004024  0.000824 -0.003350  \n",
      "24  0.001935 -0.000400 -0.003496  \n",
      "25  0.000816 -0.000649 -0.002544  \n",
      "26  0.000250 -0.000487 -0.001317  \n",
      "27 -0.000009 -0.000217 -0.000251  \n",
      "28 -0.000091  0.000032  0.000500  \n",
      "29 -0.000071  0.000224  0.000939  \n",
      "30  0.000003  0.000355  0.001130  \n",
      "31  0.000095  0.000429  0.001142  \n",
      "32  0.000176  0.000449  0.001029  \n",
      "33  0.000226  0.000417  0.000832  \n",
      "34  0.000233  0.000338  0.000584  \n",
      "35  0.000199  0.000221  0.000309  \n",
      "36  0.000131  0.000082  0.000036  \n",
      "37  0.000043 -0.000061 -0.000210  \n",
      "38 -0.000049 -0.000189 -0.000407  \n",
      "39 -0.000128 -0.000286 -0.000536  \n",
      "40 -0.000184 -0.000341 -0.000587  \n"
     ]
    }
   ],
   "source": [
    "nn_plot_list=compute_errors()\n",
    "nn_plot_container = np.transpose(np.reshape(nn_plot_list,(-1,41)))\n",
    "nn_plots = pd.DataFrame(nn_plot_container)\n",
    "nn_plots=nn_plots.rename(columns={0: 'T=0',1 : 'T=0.1111',2 : 'T=0.2222',3 : 'T=0.3333',4 : 'T=0.4444',5 : 'T=0.5555',6 : 'T=0.6666',7 : 'T=0.7777',8 : 'T=0.8888',9 : 'T=1'})\n",
    "print(nn_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgyU5qYhjUgO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Simpflied American put.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
