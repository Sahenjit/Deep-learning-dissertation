{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.stats import norm\n",
    "import pylab\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_state = 1\n",
    "\n",
    "riskfree_rate = 0.05\n",
    "volatility = 0.25\n",
    "strike_price = 0.5\n",
    "\n",
    "# Time limits\n",
    "T_min = 0.\n",
    "T_max  = 1.\n",
    "\n",
    "# Space limits\n",
    "S_min = 1e-10 \n",
    "S_max = 1.\n",
    "\n",
    "# Network parameters\n",
    "nr_layers = 3\n",
    "nr_nodes_per_layer = 50\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate_decay_steps = 10000\n",
    "learning_rate_decay_rate = 0.9\n",
    "\n",
    "# Training parameters\n",
    "steps_per_sample = 1\n",
    "nr_epochs = 8000\n",
    "\n",
    "# Number of samples\n",
    "N_interior = 1000\n",
    "N_initial = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGM neural network model\n",
    "class DNN(tf.keras.Model):#creating a class called DNN\n",
    "    def __init__(self, nr_layers, nr_nodes_each_layer, state_dimension=1):#init is similiar to a constructor in c++\n",
    "        #self allows you to call instances of that class similar to a this pointer.\n",
    "        tf.keras.Model.__init__(self)#calls the parent constructor\n",
    "        self.nr_layers = nr_layers #assinging member variables nr_layers as input varaible \n",
    "\n",
    "        self.initial_layer = DenseLayer(state_dimension + 1, nr_nodes_each_layer, activation=tf.nn.tanh)\n",
    "        #setting parameters for initial layers start from 2(state dimension +time(1)) nodes all the way to 50 nodes\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(nr_layers): #iterating over the 3 layers\n",
    "            self.hidden_layers.append(LayerFromPaper(state_dimension + 1, nr_nodes_each_layer, activation=tf.nn.tanh))#appending the hidden layers create 3 of them\n",
    "        self.final_layer = DenseLayer(nr_nodes_each_layer, 1, activation=None)#create the last layer\n",
    "\n",
    "\n",
    "    def call(self, t, x):# creating of a member function\n",
    "        X = tf.concat([t,x], 1) # concats the time and stock price in columns\n",
    "\n",
    "        S = self.initial_layer.call(X)#call is a member function of dense layer\n",
    "        for i in range(self.nr_layers):\n",
    "            S = self.hidden_layers[i].call({'S': S, 'X': X})#creating the hidden layers, #X=time and asset price we concat this in  X = tf.concat([t,x], 1)\n",
    "        result = self.final_layer.call(S)#creating the final layer\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "# Neural network layers\n",
    "\n",
    "class DenseLayer(tf.keras.layers.Layer):# creating the class Dense layers\n",
    "    def __init__(self, nr_inputs, nr_outputs, activation): #creating the constructor for that class\n",
    "        tf.keras.layers.Layer.__init__(self)#initialzing the object of that class\n",
    "        \n",
    "        self.initializer = tf.keras.initializers.glorot_normal\n",
    "        #self.initializer=tf.contrib.layers.xavier_initializer()) #TF 1\n",
    "\n",
    "        self.nr_inputs = nr_inputs# initilaizing nr_inputs as a member variable\n",
    "        self.nr_outputs = nr_outputs # initilizing nr_outputs as a member varaible\n",
    "        \n",
    "        self.W = self.add_variable(\"W\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                   initializer=self.initializer())# initializing W as a member variable creating a matrix type object in order to train it for the neural network\n",
    "        #W is one of the weights\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.nr_outputs])  #bias or constant added only at the end of training as therefore has no initilization\n",
    "\n",
    "        self.activation = activation #saving it as a member variable\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):#member function of Dense Layer\n",
    "        S = tf.add(tf.matmul(inputs, self.W), self.b) #From paper\n",
    "        if not self.activation == None:\n",
    "            S = self.activation(S) #activation function with the sigma (not volatility)\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "\n",
    "class LayerFromPaper(tf.keras.layers.Layer):\n",
    "    def __init__(self, nr_inputs, nr_outputs, activation):\n",
    "        tf.keras.layers.Layer.__init__(self)\n",
    "\n",
    "        self.initializer = tf.keras.initializers.glorot_normal\n",
    "        #self.initializer=tf.contrib.layers.xavier_initializer()) #TF 1\n",
    "        \n",
    "        self.nr_outputs = nr_outputs\n",
    "        self.nr_inputs = nr_inputs\n",
    "\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.nr_inputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.nr_outputs, self.nr_outputs],\n",
    "                                    initializer=self.initializer())\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.nr_outputs])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.nr_outputs])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.nr_outputs])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.nr_outputs])\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        S = inputs['S']\n",
    "        X = inputs['X']\n",
    "\n",
    "        Z = self.activation(tf.add(tf.add(tf.matmul(X, self.Uz), tf.matmul(S, self.Wz)), self.bz))\n",
    "        G = self.activation(tf.add(tf.add(tf.matmul(X, self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.activation(tf.add(tf.add(tf.matmul(X, self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        H = self.activation(tf.add(tf.add(tf.matmul(X, self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        Snew = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z, S))\n",
    "\n",
    "        return Snew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def get_residual(model, t_interior, x_interior, t_initial, x_initial):#model =BS Model,t_interior=time from 0 to 1,\n",
    "    #x_interior=stock price from min to max, t_initial =t_min, x_initial=S_min\n",
    "    # Loss term #1: PDE\n",
    "    V = model(t_interior, x_interior)# valuation of the neural network\n",
    "    V_t = tf.gradients(V, t_interior)[0] # DV/DT\n",
    "    V_x = tf.gradients(V, x_interior)[0] # DV/DS\n",
    "    V_xx = tf.gradients(V_x, x_interior)[0] #D^2V/DS^2\n",
    "    #black scholes formula\n",
    "    f = -V_t + riskfree_rate * V -riskfree_rate*x_interior*V_x - 0.5*volatility**2 * x_interior**2 * V_xx\n",
    "\n",
    "    L1 = tf.reduce_mean(tf.square(f)) #mean of the squared residuals, residuals of the PDE J(F) part 1\n",
    "    #Payoff function\n",
    "    payoff= tf.math.maximum(0.,tf.subtract(x_interior,strike_price))\n",
    "    #L2 norm\n",
    "    L2=tf.reduce_mean(tf.square(tf.math.maximum(0.,payoff-V)))#J(f) part 2\n",
    "    #max deviation of L2\n",
    "    Max_dev=tf.math.reduce_max(tf.math.maximum(0.,payoff-V))\n",
    "    #min deviation of L2\n",
    "    Min_dev=tf.math.reduce_min(tf.math.maximum(0.,payoff-V))\n",
    "    # Loss term #3: initial/terminal condition\n",
    "    L3 = tf.reduce_mean(tf.square(model(t_initial,x_initial) - tf.math.maximum(0., x_initial - strike_price))) # J(F) part 3\n",
    "\n",
    "    return (Max_dev,L1,L2,L3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Sampling\n",
    "def get_monte_carlo_points(N_interior, N_initial):\n",
    "    # Sampler #1: PDE domain\n",
    "    t_interior = np.random.uniform(low=T_min - 0.5*(T_max - T_min),\n",
    "                           high=T_max,\n",
    "                           size=[N_interior,1])\n",
    "    s_interior = np.random.uniform(low=S_min - (S_max - S_min)*0.5,\n",
    "                           high=S_max + (S_max - S_min)*0.5,\n",
    "                           size=[N_interior,1])\n",
    "    #you take all the t and the state space\n",
    "    \n",
    "    # Sampler #2: initial/terminal condition\n",
    "    t_initial = T_max * np.ones((N_initial,1)) #Terminal condition\n",
    "    s_initial = np.random.uniform(low=S_min - (S_max - S_min)*0.5,\n",
    "                           high=S_max + (S_max - S_min)*0.5,\n",
    "                           size=[N_initial,1])\n",
    "    \n",
    "    return (t_interior, s_interior, t_initial, s_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_dimensional_bs_solution(time, state, strike_price, volatility, riskfree_rate):\n",
    "    drift = riskfree_rate\n",
    "    if np.size(volatility) == 1:  # scalar sigma\n",
    "        volatility = volatility * np.ones(np.shape(time))\n",
    "    if np.size(strike_price) == 1:  # scalar sigma\n",
    "        strike_price = strike_price * np.ones(np.shape(time))\n",
    "    if np.size(drift) == 1:  # scalar sigma\n",
    "        drift = drift * np.ones(np.shape(time))\n",
    "    if np.size(riskfree_rate) == 1:  # scalar sigma\n",
    "        riskfree_rate = riskfree_rate * np.ones(np.shape(time))\n",
    "\n",
    "    solution = np.zeros(np.shape(time))\n",
    "    d1 = np.zeros(np.shape(time))\n",
    "    d2 = np.zeros(np.shape(time))\n",
    "\n",
    "    is_initial_time = (time == 0)\n",
    "    is_zero = (state == 0)\n",
    "    is_not_special_case = (time > 0) & (state > 0)\n",
    "\n",
    "    d1[is_not_special_case] = 1. / (volatility[is_not_special_case] * np.sqrt(time[is_not_special_case])) * (\n",
    "            np.log(state[is_not_special_case] / strike_price[is_not_special_case]) + (\n",
    "            drift[is_not_special_case] + volatility[is_not_special_case] ** 2 * 0.5) * time[is_not_special_case])\n",
    "    d2[is_not_special_case] = d1[is_not_special_case] \\\n",
    "                              - volatility[is_not_special_case] * np.sqrt(time[is_not_special_case])\n",
    "\n",
    "    solution[is_not_special_case] = state[is_not_special_case] * norm.cdf(d1[is_not_special_case]) - strike_price[\n",
    "        is_not_special_case] * np.exp(-riskfree_rate[is_not_special_case] * time[is_not_special_case]) * norm.cdf(\n",
    "        d2[is_not_special_case])\n",
    "\n",
    "    solution[is_initial_time] = np.maximum(0, state[is_initial_time] - strike_price[is_initial_time])\n",
    "    solution[is_zero] = 0.\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_plot_list=[]\n",
    "def compute_errors():\n",
    "    max_error_container=[]\n",
    "    mean_error_container=[]\n",
    "    max_error=0\n",
    "    times_to_evaluate_error=np.linspace(T_min,T_max,10)\n",
    "    for t in times_to_evaluate_error:\n",
    "        tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "        nn_plot, = sess.run([vplot_t],\n",
    "                        feed_dict={tplot_t:tt, xplot_t:xplot.reshape(-1,1)})\n",
    "        nn_plot_list.append(nn_plot,)\n",
    "        exact_plot = one_dimensional_bs_solution(\n",
    "            T_max-tt, xplot.reshape(-1,1), strike_price, volatility, riskfree_rate)\n",
    "        max_error=np.maximum(np.max(np.abs(nn_plot-exact_plot)),max_error) \n",
    "        mean_error_container.append(np.mean(np.abs(nn_plot-exact_plot)))\n",
    "    New_mean_error_container=np.mean(mean_error_container)\n",
    "    \n",
    "    return max_error,New_mean_error_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "This is \n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "This is \n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/shankar/opt/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <__main__.DNN object at 0x7ff3691a6198>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "# Neural Network definition\n",
    "\n",
    "model = DNN(nr_layers, nr_nodes_per_layer) #first time this model is constructed\n",
    "\n",
    "t_interior_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"time_interior\") #allows you to fill it with numbers\n",
    "x_interior_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, dimension_state), name=\"stock_prices_interior\")\n",
    "t_initial_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"time_initial\")#allows you to fill it with numbers\n",
    "x_initial_tf = tf.compat.v1.placeholder(tf.float32, shape=(None, dimension_state), name=\"stock_prices_initial\")\n",
    "\n",
    "\n",
    "Max_dev,residual_interior,residual_exterior,residual_initial= get_residual(model, t_interior_tf, x_interior_tf, t_initial_tf, x_initial_tf)\n",
    "print(\"This is \")\n",
    "print(residual_exterior)\n",
    "print(\"This is \")\n",
    "print(residual_interior)\n",
    "residual = residual_interior + residual_initial+(residual_exterior*1) #this residual is the combination of the L1 and L2 norm\n",
    "# Optimizer parameters\n",
    "nr_steps = tf.Variable(0, trainable=False) #very weird itertation counter, counts the no of optimization steps we took\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(initial_learning_rate, nr_steps,\n",
    "                                           learning_rate_decay_steps, \n",
    "                                           learning_rate_decay_rate, staircase=True)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(residual) #defining the optimizer \n",
    "# gradient descent with a momemtum term. mizimised the residuals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot tensors\n",
    "tplot_t = tf.compat.v1.placeholder(tf.float32, [None,1], name=\"tplot_t\") # We name to recover it later\n",
    "xplot_t = tf.compat.v1.placeholder(tf.float32, [None,1], name=\"xplot_t\")\n",
    "vplot_t = tf.identity(model(tplot_t, xplot_t), name=\"vplot_t\") # Trick for naming the trained model\n",
    "\n",
    "\n",
    "# Training data holders\n",
    "residuals_list_residual0_1=[]\n",
    "residuals_list_residual1_1=[]\n",
    "residuals_list_residual2_1=[]\n",
    "residuals_list_residual3_1=[]\n",
    "residuals_list_residual4_1=[]\n",
    "residuals_list_MaxDev0_1=[]\n",
    "residuals_list_MaxDev1_1=[]\n",
    "residuals_list_MaxDev2_1=[]\n",
    "residuals_list_MaxDev3_1=[]\n",
    "residuals_list_MaxDev4_1=[]\n",
    "residuals_list_L10_1=[]\n",
    "residuals_list_L11_1=[]\n",
    "residuals_list_L12_1=[]\n",
    "residuals_list_L13_1=[]\n",
    "residuals_list_L14_1=[]\n",
    "residuals_list_L20_1=[]\n",
    "residuals_list_L21_1=[]\n",
    "residuals_list_L22_1=[]\n",
    "residuals_list_L23_1=[]\n",
    "residuals_list_L24_1=[]\n",
    "residuals_list_L30_1=[]\n",
    "residuals_list_L31_1=[]\n",
    "residuals_list_L32_1=[]\n",
    "residuals_list_L33_1=[]\n",
    "residuals_list_L34_1=[]\n",
    "# Train network!!\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "This is the first run of the neural network\n",
      "Stage: 0000, Loss: 1.349727e+00, Maximum_Deviation: 1.362720e+00, L1: 4.633159e-02, L2: 5.375406e-01, L3: 7.658548e-01\n",
      "Stage: 0100, Loss: 7.832768e-04, Maximum_Deviation: 2.417240e-02, L1: 1.975694e-04, L2: 4.371129e-05, L3: 5.419962e-04\n",
      "Stage: 0200, Loss: 5.626092e-04, Maximum_Deviation: 4.252791e-02, L1: 7.245746e-05, L2: 3.476651e-05, L3: 4.553852e-04\n",
      "Stage: 0300, Loss: 4.265458e-04, Maximum_Deviation: 4.451060e-02, L1: 5.872343e-05, L2: 3.543538e-05, L3: 3.323870e-04\n",
      "Stage: 0400, Loss: 4.634018e-04, Maximum_Deviation: 4.177070e-02, L1: 6.492058e-05, L2: 3.488760e-05, L3: 3.635937e-04\n",
      "Stage: 0500, Loss: 3.543728e-04, Maximum_Deviation: 4.157871e-02, L1: 4.441339e-05, L2: 2.794028e-05, L3: 2.820192e-04\n",
      "Stage: 0600, Loss: 3.428472e-04, Maximum_Deviation: 3.397340e-02, L1: 3.888392e-05, L2: 2.429875e-05, L3: 2.796645e-04\n",
      "Stage: 0700, Loss: 2.415922e-04, Maximum_Deviation: 2.280301e-02, L1: 3.513021e-05, L2: 2.571413e-05, L3: 1.807478e-04\n",
      "Stage: 0800, Loss: 1.667157e-04, Maximum_Deviation: 2.005851e-02, L1: 2.364084e-05, L2: 1.598725e-05, L3: 1.270877e-04\n",
      "Stage: 0900, Loss: 2.943530e-04, Maximum_Deviation: 2.008116e-02, L1: 2.690941e-05, L2: 1.712744e-05, L3: 2.503162e-04\n",
      "Stage: 1000, Loss: 2.006505e-04, Maximum_Deviation: 2.148125e-02, L1: 3.163595e-05, L2: 2.369543e-05, L3: 1.453191e-04\n",
      "Stage: 1100, Loss: 2.255405e-04, Maximum_Deviation: 2.383339e-02, L1: 1.221171e-05, L2: 1.851924e-05, L3: 1.948096e-04\n",
      "Stage: 1200, Loss: 1.976893e-04, Maximum_Deviation: 2.100320e-02, L1: 1.368178e-05, L2: 1.726830e-05, L3: 1.667392e-04\n",
      "Stage: 1300, Loss: 1.485348e-04, Maximum_Deviation: 1.753125e-02, L1: 1.256284e-05, L2: 1.579205e-05, L3: 1.201799e-04\n",
      "Stage: 1400, Loss: 2.174415e-04, Maximum_Deviation: 1.640949e-02, L1: 1.602100e-05, L2: 1.161161e-05, L3: 1.898089e-04\n",
      "Stage: 1500, Loss: 1.600187e-04, Maximum_Deviation: 1.408756e-02, L1: 1.269952e-05, L2: 8.267615e-06, L3: 1.390516e-04\n",
      "Stage: 1600, Loss: 1.161496e-04, Maximum_Deviation: 1.364173e-02, L1: 1.247264e-05, L2: 1.035401e-05, L3: 9.332297e-05\n",
      "Stage: 1700, Loss: 9.122340e-05, Maximum_Deviation: 1.306073e-02, L1: 1.878903e-05, L2: 6.551355e-06, L3: 6.588302e-05\n",
      "Stage: 1800, Loss: 1.254816e-04, Maximum_Deviation: 9.377569e-03, L1: 1.401111e-05, L2: 2.234326e-06, L3: 1.092362e-04\n",
      "Stage: 1900, Loss: 1.097878e-04, Maximum_Deviation: 1.132673e-02, L1: 1.254099e-05, L2: 4.738139e-06, L3: 9.250869e-05\n",
      "Stage: 2000, Loss: 7.197045e-05, Maximum_Deviation: 8.887962e-03, L1: 1.273607e-05, L2: 3.367061e-06, L3: 5.586731e-05\n",
      "Stage: 2100, Loss: 7.765367e-05, Maximum_Deviation: 1.089437e-02, L1: 1.150388e-05, L2: 3.000858e-06, L3: 6.314892e-05\n",
      "Stage: 2200, Loss: 7.109176e-05, Maximum_Deviation: 9.439051e-03, L1: 1.014041e-05, L2: 3.602773e-06, L3: 5.734858e-05\n",
      "Stage: 2300, Loss: 7.749946e-05, Maximum_Deviation: 9.296909e-03, L1: 9.579314e-06, L2: 2.064096e-06, L3: 6.585605e-05\n",
      "Stage: 2400, Loss: 7.032576e-05, Maximum_Deviation: 1.230647e-02, L1: 1.372038e-05, L2: 3.952103e-06, L3: 5.265327e-05\n",
      "Stage: 2500, Loss: 9.956339e-05, Maximum_Deviation: 1.004945e-02, L1: 6.863396e-06, L2: 1.731625e-06, L3: 9.096837e-05\n",
      "Stage: 2600, Loss: 4.970528e-05, Maximum_Deviation: 6.942242e-03, L1: 6.288653e-06, L2: 1.264005e-06, L3: 4.215262e-05\n",
      "Stage: 2700, Loss: 6.555407e-05, Maximum_Deviation: 8.176565e-03, L1: 1.017424e-05, L2: 9.452915e-07, L3: 5.443454e-05\n",
      "Stage: 2800, Loss: 5.755912e-05, Maximum_Deviation: 1.074314e-02, L1: 7.038339e-06, L2: 2.890914e-06, L3: 4.762987e-05\n",
      "Stage: 2900, Loss: 6.518247e-05, Maximum_Deviation: 6.778672e-03, L1: 4.205418e-06, L2: 9.825688e-07, L3: 5.999449e-05\n",
      "Stage: 3000, Loss: 6.113288e-05, Maximum_Deviation: 9.387553e-03, L1: 1.247916e-05, L2: 2.413233e-06, L3: 4.624048e-05\n",
      "Stage: 3100, Loss: 4.429436e-05, Maximum_Deviation: 6.692544e-03, L1: 2.858281e-06, L2: 9.336874e-07, L3: 4.050239e-05\n",
      "Stage: 3200, Loss: 3.873204e-05, Maximum_Deviation: 9.295613e-03, L1: 8.403091e-06, L2: 2.554427e-06, L3: 2.777452e-05\n",
      "Stage: 3300, Loss: 3.852338e-05, Maximum_Deviation: 5.187884e-03, L1: 4.261075e-06, L2: 4.327999e-07, L3: 3.382950e-05\n",
      "Stage: 3400, Loss: 7.936605e-05, Maximum_Deviation: 4.371285e-03, L1: 5.583875e-06, L2: 2.161193e-07, L3: 7.356606e-05\n",
      "Stage: 3500, Loss: 3.642165e-05, Maximum_Deviation: 7.746637e-03, L1: 3.628484e-06, L2: 1.132008e-06, L3: 3.166115e-05\n",
      "Stage: 3600, Loss: 3.302476e-05, Maximum_Deviation: 9.150028e-03, L1: 3.867821e-06, L2: 1.313015e-06, L3: 2.784392e-05\n",
      "Stage: 3700, Loss: 1.715308e-05, Maximum_Deviation: 6.699383e-03, L1: 3.617252e-06, L2: 1.138704e-06, L3: 1.239712e-05\n",
      "Stage: 3800, Loss: 1.294719e-05, Maximum_Deviation: 5.085111e-03, L1: 3.901783e-06, L2: 2.902943e-07, L3: 8.755113e-06\n",
      "Stage: 3900, Loss: 3.373406e-05, Maximum_Deviation: 8.656576e-03, L1: 4.443640e-06, L2: 2.534425e-06, L3: 2.675599e-05\n",
      "Stage: 4000, Loss: 3.050684e-05, Maximum_Deviation: 1.003918e-02, L1: 5.585842e-06, L2: 2.053383e-06, L3: 2.286762e-05\n",
      "Stage: 4100, Loss: 2.918400e-05, Maximum_Deviation: 7.750914e-03, L1: 5.084770e-06, L2: 1.315396e-06, L3: 2.278384e-05\n",
      "Stage: 4200, Loss: 4.258490e-05, Maximum_Deviation: 9.307995e-03, L1: 1.169491e-05, L2: 2.295084e-06, L3: 2.859491e-05\n",
      "Stage: 4300, Loss: 1.355878e-05, Maximum_Deviation: 4.817605e-03, L1: 1.971856e-06, L2: 3.739356e-07, L3: 1.121299e-05\n",
      "Stage: 4400, Loss: 2.142175e-05, Maximum_Deviation: 4.299313e-03, L1: 5.026882e-06, L2: 2.457724e-07, L3: 1.614910e-05\n",
      "Stage: 4500, Loss: 2.443969e-05, Maximum_Deviation: 3.795117e-03, L1: 6.011648e-06, L2: 1.297018e-07, L3: 1.829835e-05\n",
      "Stage: 4600, Loss: 2.990204e-05, Maximum_Deviation: 8.104160e-03, L1: 6.143892e-06, L2: 9.910921e-07, L3: 2.276706e-05\n",
      "Stage: 4700, Loss: 9.686124e-05, Maximum_Deviation: 2.991229e-03, L1: 1.336782e-05, L2: 1.140115e-07, L3: 8.337941e-05\n",
      "Stage: 4800, Loss: 1.836737e-05, Maximum_Deviation: 5.044043e-03, L1: 3.191233e-06, L2: 3.953317e-07, L3: 1.478081e-05\n",
      "Stage: 4900, Loss: 1.397679e-05, Maximum_Deviation: 6.198093e-03, L1: 2.228667e-06, L2: 4.879575e-07, L3: 1.126017e-05\n",
      "Stage: 5000, Loss: 7.331317e-05, Maximum_Deviation: 3.698304e-03, L1: 5.187252e-05, L2: 6.156856e-08, L3: 2.137908e-05\n",
      "Stage: 5100, Loss: 7.327938e-06, Maximum_Deviation: 3.807783e-03, L1: 1.505638e-06, L2: 9.751698e-08, L3: 5.724783e-06\n",
      "Stage: 5200, Loss: 1.584538e-05, Maximum_Deviation: 4.933029e-03, L1: 1.661841e-06, L2: 1.298291e-07, L3: 1.405371e-05\n",
      "Stage: 5300, Loss: 1.616455e-05, Maximum_Deviation: 5.215079e-03, L1: 4.699112e-06, L2: 3.579165e-07, L3: 1.110752e-05\n",
      "Stage: 5400, Loss: 2.371617e-05, Maximum_Deviation: 3.509030e-03, L1: 1.360171e-05, L2: 1.617901e-07, L3: 9.952665e-06\n",
      "Stage: 5500, Loss: 8.554716e-06, Maximum_Deviation: 5.163863e-03, L1: 1.291394e-06, L2: 1.332965e-07, L3: 7.130026e-06\n",
      "Stage: 5600, Loss: 9.577826e-06, Maximum_Deviation: 4.160747e-03, L1: 2.946349e-06, L2: 6.061403e-08, L3: 6.570863e-06\n",
      "Stage: 5700, Loss: 1.131353e-05, Maximum_Deviation: 3.225327e-03, L1: 1.076704e-06, L2: 1.928584e-08, L3: 1.021754e-05\n",
      "Stage: 5800, Loss: 7.462874e-06, Maximum_Deviation: 4.632771e-03, L1: 1.251515e-06, L2: 9.510087e-08, L3: 6.116258e-06\n",
      "Stage: 5900, Loss: 9.900376e-06, Maximum_Deviation: 3.625572e-03, L1: 2.045123e-06, L2: 9.678325e-08, L3: 7.758469e-06\n",
      "Stage: 6000, Loss: 1.438046e-05, Maximum_Deviation: 2.828687e-03, L1: 7.252978e-06, L2: 1.009323e-07, L3: 7.026551e-06\n",
      "Stage: 6100, Loss: 1.517430e-05, Maximum_Deviation: 4.236519e-03, L1: 8.624807e-06, L2: 1.361266e-07, L3: 6.413363e-06\n",
      "Stage: 6200, Loss: 6.282402e-06, Maximum_Deviation: 4.731566e-03, L1: 1.592746e-06, L2: 4.234484e-07, L3: 4.266208e-06\n",
      "Stage: 6300, Loss: 1.281384e-05, Maximum_Deviation: 2.722576e-03, L1: 3.727121e-06, L2: 2.491922e-08, L3: 9.061795e-06\n",
      "Stage: 6400, Loss: 1.861197e-05, Maximum_Deviation: 6.145254e-03, L1: 6.881925e-06, L2: 3.878244e-07, L3: 1.134222e-05\n",
      "Stage: 6500, Loss: 2.865295e-05, Maximum_Deviation: 5.856186e-03, L1: 1.269449e-05, L2: 1.014809e-06, L3: 1.494365e-05\n",
      "Stage: 6600, Loss: 5.268184e-05, Maximum_Deviation: 2.081707e-03, L1: 1.040541e-05, L2: 7.459792e-09, L3: 4.226897e-05\n",
      "Stage: 6700, Loss: 1.161300e-05, Maximum_Deviation: 6.166503e-03, L1: 1.527318e-06, L2: 5.304015e-07, L3: 9.555278e-06\n",
      "Stage: 6800, Loss: 1.502672e-05, Maximum_Deviation: 5.238742e-03, L1: 4.872873e-06, L2: 2.163137e-07, L3: 9.937533e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 6900, Loss: 1.238703e-05, Maximum_Deviation: 1.571015e-03, L1: 4.108279e-06, L2: 1.421926e-08, L3: 8.264530e-06\n",
      "Stage: 7000, Loss: 7.112174e-06, Maximum_Deviation: 1.370296e-03, L1: 2.343037e-06, L2: 3.536756e-09, L3: 4.765600e-06\n",
      "Stage: 7100, Loss: 1.390249e-04, Maximum_Deviation: 1.634806e-02, L1: 2.654528e-05, L2: 9.406733e-06, L3: 1.030729e-04\n",
      "Stage: 7200, Loss: 3.062675e-06, Maximum_Deviation: 4.170507e-03, L1: 8.478077e-07, L2: 5.799192e-08, L3: 2.156875e-06\n",
      "Stage: 7300, Loss: 4.277876e-06, Maximum_Deviation: 3.894448e-03, L1: 1.713786e-06, L2: 1.070111e-07, L3: 2.457079e-06\n",
      "Stage: 7400, Loss: 4.510183e-06, Maximum_Deviation: 4.338667e-03, L1: 1.697117e-06, L2: 7.978684e-08, L3: 2.733279e-06\n",
      "Stage: 7500, Loss: 4.225855e-06, Maximum_Deviation: 3.761932e-03, L1: 1.494901e-06, L2: 6.264194e-08, L3: 2.668313e-06\n",
      "Stage: 7600, Loss: 3.599525e-06, Maximum_Deviation: 2.115414e-03, L1: 1.892776e-06, L2: 3.202932e-08, L3: 1.674719e-06\n",
      "Stage: 7700, Loss: 6.994186e-06, Maximum_Deviation: 7.163435e-04, L1: 2.002937e-06, L2: 1.710676e-09, L3: 4.989538e-06\n",
      "Stage: 7800, Loss: 4.609287e-06, Maximum_Deviation: 3.042564e-03, L1: 9.623553e-07, L2: 1.556571e-08, L3: 3.631366e-06\n",
      "Stage: 7900, Loss: 2.837530e-05, Maximum_Deviation: 1.403034e-03, L1: 5.810640e-06, L2: 4.118570e-08, L3: 2.252348e-05\n",
      "Stage: 7999, Loss: 6.866957e-06, Maximum_Deviation: 4.260346e-03, L1: 1.555061e-06, L2: 9.680692e-08, L3: 5.215089e-06\n",
      "1\n",
      "This is the second run of the neural network\n",
      "Stage: 0000, Loss: 1.082060e+00, Maximum_Deviation: 1.702415e-01, L1: 4.754977e-02, L2: 4.233426e-04, L3: 1.034087e+00\n",
      "Stage: 0100, Loss: 1.357854e-03, Maximum_Deviation: 3.666168e-02, L1: 2.057283e-04, L2: 7.751545e-05, L3: 1.074610e-03\n",
      "Stage: 0200, Loss: 7.132468e-04, Maximum_Deviation: 3.062332e-02, L1: 6.768810e-05, L2: 4.512836e-05, L3: 6.004303e-04\n",
      "Stage: 0300, Loss: 5.591391e-04, Maximum_Deviation: 2.140653e-02, L1: 4.939956e-05, L2: 4.706820e-05, L3: 4.626712e-04\n",
      "Stage: 0400, Loss: 4.487322e-04, Maximum_Deviation: 2.175415e-02, L1: 6.432136e-05, L2: 4.692811e-05, L3: 3.374827e-04\n",
      "Stage: 0500, Loss: 4.360590e-04, Maximum_Deviation: 3.134894e-02, L1: 5.496832e-05, L2: 3.541288e-05, L3: 3.456778e-04\n",
      "Stage: 0600, Loss: 2.873587e-04, Maximum_Deviation: 3.269345e-02, L1: 4.286844e-05, L2: 3.647411e-05, L3: 2.080161e-04\n",
      "Stage: 0700, Loss: 3.279207e-04, Maximum_Deviation: 1.595497e-02, L1: 3.781213e-05, L2: 1.621377e-05, L3: 2.738948e-04\n",
      "Stage: 0800, Loss: 3.656303e-04, Maximum_Deviation: 1.514721e-02, L1: 3.099268e-05, L2: 1.131075e-05, L3: 3.233269e-04\n",
      "Stage: 0900, Loss: 1.740701e-04, Maximum_Deviation: 1.830733e-02, L1: 2.690326e-05, L2: 1.656162e-05, L3: 1.306052e-04\n",
      "Stage: 1000, Loss: 1.825221e-04, Maximum_Deviation: 1.672512e-02, L1: 3.328845e-05, L2: 1.552972e-05, L3: 1.337039e-04\n",
      "Stage: 1100, Loss: 2.443512e-04, Maximum_Deviation: 1.558280e-02, L1: 2.997874e-05, L2: 1.232422e-05, L3: 2.020483e-04\n",
      "Stage: 1200, Loss: 2.121668e-04, Maximum_Deviation: 1.766467e-02, L1: 1.822766e-05, L2: 8.423219e-06, L3: 1.855159e-04\n",
      "Stage: 1300, Loss: 1.469265e-04, Maximum_Deviation: 2.116919e-02, L1: 2.111286e-05, L2: 2.352258e-05, L3: 1.022911e-04\n",
      "Stage: 1400, Loss: 1.800740e-04, Maximum_Deviation: 1.455331e-02, L1: 1.890609e-05, L2: 8.795673e-06, L3: 1.523722e-04\n",
      "Stage: 1500, Loss: 1.263486e-04, Maximum_Deviation: 1.599133e-02, L1: 1.933320e-05, L2: 5.475823e-06, L3: 1.015395e-04\n",
      "Stage: 1600, Loss: 1.497733e-04, Maximum_Deviation: 1.347101e-02, L1: 1.981016e-05, L2: 1.173159e-05, L3: 1.182316e-04\n",
      "Stage: 1700, Loss: 1.576141e-04, Maximum_Deviation: 1.077056e-02, L1: 3.815088e-05, L2: 8.512402e-06, L3: 1.109508e-04\n",
      "Stage: 1800, Loss: 1.210351e-04, Maximum_Deviation: 1.471138e-02, L1: 2.826317e-05, L2: 7.724093e-06, L3: 8.504785e-05\n",
      "Stage: 1900, Loss: 8.167983e-05, Maximum_Deviation: 9.035289e-03, L1: 1.841491e-05, L2: 1.995252e-06, L3: 6.126967e-05\n",
      "Stage: 2000, Loss: 7.753707e-05, Maximum_Deviation: 7.220030e-03, L1: 2.112402e-05, L2: 2.082829e-06, L3: 5.433023e-05\n",
      "Stage: 2100, Loss: 9.340386e-05, Maximum_Deviation: 1.032710e-02, L1: 1.642338e-05, L2: 2.741632e-06, L3: 7.423885e-05\n",
      "Stage: 2200, Loss: 2.103085e-04, Maximum_Deviation: 1.579320e-02, L1: 6.972714e-05, L2: 6.925300e-06, L3: 1.336560e-04\n",
      "Stage: 2300, Loss: 1.283762e-04, Maximum_Deviation: 5.467534e-03, L1: 2.743610e-05, L2: 1.324015e-06, L3: 9.961612e-05\n",
      "Stage: 2400, Loss: 8.453630e-05, Maximum_Deviation: 1.422536e-02, L1: 1.848870e-05, L2: 5.272375e-06, L3: 6.077522e-05\n",
      "Stage: 2500, Loss: 1.031368e-04, Maximum_Deviation: 5.915761e-03, L1: 1.452826e-05, L2: 9.128180e-07, L3: 8.769575e-05\n",
      "Stage: 2600, Loss: 6.272169e-05, Maximum_Deviation: 9.213269e-03, L1: 1.120381e-05, L2: 2.545023e-06, L3: 4.897286e-05\n",
      "Stage: 2700, Loss: 8.106907e-05, Maximum_Deviation: 9.988666e-03, L1: 2.808091e-05, L2: 3.399441e-06, L3: 4.958872e-05\n",
      "Stage: 2800, Loss: 8.759684e-05, Maximum_Deviation: 1.535124e-02, L1: 1.794181e-05, L2: 5.832611e-06, L3: 6.382242e-05\n",
      "Stage: 2900, Loss: 5.305521e-05, Maximum_Deviation: 7.637024e-03, L1: 9.148863e-06, L2: 1.776804e-06, L3: 4.212954e-05\n",
      "Stage: 3000, Loss: 2.019004e-04, Maximum_Deviation: 4.646301e-03, L1: 8.348845e-05, L2: 4.943035e-07, L3: 1.179177e-04\n",
      "Stage: 3100, Loss: 5.729260e-05, Maximum_Deviation: 9.291470e-03, L1: 9.892018e-06, L2: 3.100385e-06, L3: 4.430020e-05\n",
      "Stage: 3200, Loss: 2.882393e-05, Maximum_Deviation: 7.150769e-03, L1: 5.543598e-06, L2: 1.981266e-06, L3: 2.129906e-05\n",
      "Stage: 3300, Loss: 5.245459e-05, Maximum_Deviation: 1.118493e-02, L1: 6.430382e-06, L2: 2.134981e-06, L3: 4.388922e-05\n",
      "Stage: 3400, Loss: 5.990655e-05, Maximum_Deviation: 1.156497e-02, L1: 1.781584e-05, L2: 5.486139e-06, L3: 3.660457e-05\n",
      "Stage: 3500, Loss: 3.648056e-05, Maximum_Deviation: 6.369650e-03, L1: 1.273136e-05, L2: 1.016034e-06, L3: 2.273317e-05\n",
      "Stage: 3600, Loss: 3.355100e-05, Maximum_Deviation: 7.618904e-03, L1: 5.426886e-06, L2: 1.102934e-06, L3: 2.702119e-05\n",
      "Stage: 3700, Loss: 4.816648e-05, Maximum_Deviation: 3.611088e-03, L1: 6.900272e-06, L2: 1.962716e-07, L3: 4.106993e-05\n",
      "Stage: 3800, Loss: 4.699354e-05, Maximum_Deviation: 1.014817e-02, L1: 9.341692e-06, L2: 3.426439e-06, L3: 3.422541e-05\n",
      "Stage: 3900, Loss: 3.307701e-05, Maximum_Deviation: 4.668593e-03, L1: 1.074912e-05, L2: 4.764467e-07, L3: 2.185144e-05\n",
      "Stage: 4000, Loss: 2.110587e-05, Maximum_Deviation: 5.808115e-03, L1: 5.369455e-06, L2: 5.389115e-07, L3: 1.519751e-05\n",
      "Stage: 4100, Loss: 3.238945e-05, Maximum_Deviation: 5.960822e-03, L1: 5.912930e-06, L2: 7.456667e-07, L3: 2.573085e-05\n",
      "Stage: 4200, Loss: 4.436616e-05, Maximum_Deviation: 7.757545e-03, L1: 1.656288e-05, L2: 1.520180e-06, L3: 2.628310e-05\n",
      "Stage: 4300, Loss: 6.171577e-05, Maximum_Deviation: 4.275084e-03, L1: 2.087072e-05, L2: 6.041613e-07, L3: 4.024089e-05\n",
      "Stage: 4400, Loss: 2.432206e-05, Maximum_Deviation: 6.928921e-03, L1: 5.267286e-06, L2: 9.194292e-07, L3: 1.813535e-05\n",
      "Stage: 4500, Loss: 2.680206e-05, Maximum_Deviation: 6.225884e-03, L1: 9.879996e-06, L2: 1.215234e-06, L3: 1.570683e-05\n",
      "Stage: 4600, Loss: 1.662282e-05, Maximum_Deviation: 5.248427e-03, L1: 3.700076e-06, L2: 4.957082e-07, L3: 1.242704e-05\n",
      "Stage: 4700, Loss: 4.931774e-05, Maximum_Deviation: 7.351398e-03, L1: 1.359117e-05, L2: 4.572345e-06, L3: 3.115422e-05\n",
      "Stage: 4800, Loss: 2.651855e-04, Maximum_Deviation: 1.375091e-02, L1: 1.583779e-04, L2: 1.546842e-05, L3: 9.133923e-05\n",
      "Stage: 4900, Loss: 1.623446e-05, Maximum_Deviation: 5.152345e-03, L1: 4.116642e-06, L2: 5.113528e-07, L3: 1.160646e-05\n",
      "Stage: 5000, Loss: 1.625790e-05, Maximum_Deviation: 5.786657e-03, L1: 6.358023e-06, L2: 3.355772e-07, L3: 9.564299e-06\n",
      "Stage: 5100, Loss: 6.965357e-05, Maximum_Deviation: 4.979730e-03, L1: 1.276811e-05, L2: 2.402686e-07, L3: 5.664520e-05\n",
      "Stage: 5200, Loss: 7.836044e-06, Maximum_Deviation: 3.440797e-03, L1: 3.499134e-06, L2: 2.373513e-07, L3: 4.099559e-06\n",
      "Stage: 5300, Loss: 1.096832e-04, Maximum_Deviation: 8.950830e-03, L1: 5.237868e-05, L2: 4.003491e-06, L3: 5.330099e-05\n",
      "Stage: 5400, Loss: 2.293879e-05, Maximum_Deviation: 5.318642e-03, L1: 9.953895e-06, L2: 9.812710e-07, L3: 1.200362e-05\n",
      "Stage: 5500, Loss: 6.013276e-05, Maximum_Deviation: 1.558185e-03, L1: 1.165428e-05, L2: 1.022029e-08, L3: 4.846825e-05\n",
      "Stage: 5600, Loss: 1.996661e-05, Maximum_Deviation: 2.298355e-03, L1: 2.709883e-06, L2: 3.155200e-08, L3: 1.722517e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 5700, Loss: 8.290660e-05, Maximum_Deviation: 1.661539e-03, L1: 2.806636e-05, L2: 9.161382e-09, L3: 5.483108e-05\n",
      "Stage: 5800, Loss: 2.082807e-05, Maximum_Deviation: 4.797339e-03, L1: 1.244383e-05, L2: 5.072164e-07, L3: 7.877019e-06\n",
      "Stage: 5900, Loss: 9.780788e-06, Maximum_Deviation: 3.341794e-03, L1: 5.844894e-06, L2: 1.507891e-07, L3: 3.785105e-06\n",
      "Stage: 6000, Loss: 1.055969e-04, Maximum_Deviation: 3.908753e-03, L1: 8.284059e-05, L2: 1.909493e-07, L3: 2.256535e-05\n",
      "Stage: 6100, Loss: 7.990393e-06, Maximum_Deviation: 4.925251e-03, L1: 2.387787e-06, L2: 1.893285e-07, L3: 5.413278e-06\n",
      "Stage: 6200, Loss: 8.738509e-06, Maximum_Deviation: 3.999829e-03, L1: 3.329023e-06, L2: 1.491857e-07, L3: 5.260300e-06\n",
      "Stage: 6300, Loss: 1.043789e-05, Maximum_Deviation: 2.074361e-03, L1: 2.777711e-06, L2: 1.996794e-08, L3: 7.640215e-06\n",
      "Stage: 6400, Loss: 8.086488e-06, Maximum_Deviation: 4.693508e-03, L1: 1.689955e-06, L2: 5.022221e-07, L3: 5.894311e-06\n",
      "Stage: 6500, Loss: 4.478867e-05, Maximum_Deviation: 8.998275e-03, L1: 1.754765e-05, L2: 5.908312e-06, L3: 2.133271e-05\n",
      "Stage: 6600, Loss: 1.917189e-05, Maximum_Deviation: 5.500078e-03, L1: 9.077477e-06, L2: 4.423302e-07, L3: 9.652083e-06\n",
      "Stage: 6700, Loss: 1.026697e-05, Maximum_Deviation: 4.049778e-03, L1: 4.408482e-06, L2: 2.842418e-07, L3: 5.574246e-06\n",
      "Stage: 6800, Loss: 6.671332e-06, Maximum_Deviation: 3.182054e-03, L1: 2.067036e-06, L2: 1.007755e-07, L3: 4.503520e-06\n",
      "Stage: 6900, Loss: 1.620075e-05, Maximum_Deviation: 2.281964e-03, L1: 3.541210e-06, L2: 1.190397e-08, L3: 1.264763e-05\n",
      "Stage: 7000, Loss: 9.229966e-06, Maximum_Deviation: 3.628254e-03, L1: 2.775194e-06, L2: 1.129313e-07, L3: 6.341841e-06\n",
      "Stage: 7100, Loss: 1.572560e-05, Maximum_Deviation: 4.231095e-03, L1: 3.864139e-06, L2: 1.717553e-06, L3: 1.014391e-05\n",
      "Stage: 7200, Loss: 1.458175e-05, Maximum_Deviation: 3.831446e-03, L1: 6.412645e-06, L2: 5.558645e-08, L3: 8.113515e-06\n",
      "Stage: 7300, Loss: 1.147820e-05, Maximum_Deviation: 2.197623e-03, L1: 3.258136e-06, L2: 1.712123e-07, L3: 8.048849e-06\n",
      "Stage: 7400, Loss: 3.214440e-05, Maximum_Deviation: 5.998850e-03, L1: 2.820633e-05, L2: 1.856381e-07, L3: 3.752434e-06\n",
      "Stage: 7500, Loss: 1.682962e-05, Maximum_Deviation: 5.575657e-03, L1: 7.328824e-06, L2: 3.170385e-07, L3: 9.183759e-06\n",
      "Stage: 7600, Loss: 4.473171e-05, Maximum_Deviation: 0.000000e+00, L1: 9.388054e-06, L2: 0.000000e+00, L3: 3.534366e-05\n",
      "Stage: 7700, Loss: 1.669912e-05, Maximum_Deviation: 2.671361e-03, L1: 8.738724e-06, L2: 1.081925e-07, L3: 7.852200e-06\n",
      "Stage: 7800, Loss: 8.573232e-06, Maximum_Deviation: 4.941463e-03, L1: 2.952951e-06, L2: 5.126197e-07, L3: 5.107662e-06\n",
      "Stage: 7900, Loss: 5.511344e-04, Maximum_Deviation: 3.823841e-02, L1: 1.854677e-04, L2: 5.304631e-05, L3: 3.126204e-04\n",
      "Stage: 7999, Loss: 3.315955e-06, Maximum_Deviation: 3.556013e-03, L1: 1.221614e-06, L2: 4.917359e-08, L3: 2.045167e-06\n",
      "2\n",
      "This is the third run of the neural network\n",
      "Stage: 0000, Loss: 5.855965e+00, Maximum_Deviation: 2.537441e+00, L1: 1.403674e-01, L2: 3.218334e+00, L3: 2.497263e+00\n",
      "Stage: 0100, Loss: 7.659419e-04, Maximum_Deviation: 3.353250e-02, L1: 2.400024e-04, L2: 9.863359e-05, L3: 4.273059e-04\n",
      "Stage: 0200, Loss: 5.262159e-04, Maximum_Deviation: 2.148676e-02, L1: 1.098782e-04, L2: 4.241548e-05, L3: 3.739223e-04\n",
      "Stage: 0300, Loss: 4.227872e-04, Maximum_Deviation: 2.614367e-02, L1: 1.020673e-04, L2: 3.032134e-05, L3: 2.903986e-04\n",
      "Stage: 0400, Loss: 4.151484e-04, Maximum_Deviation: 2.503383e-02, L1: 8.878014e-05, L2: 2.759383e-05, L3: 2.987744e-04\n",
      "Stage: 0500, Loss: 3.555351e-04, Maximum_Deviation: 2.532053e-02, L1: 7.118649e-05, L2: 2.236021e-05, L3: 2.619884e-04\n",
      "Stage: 0600, Loss: 3.450155e-04, Maximum_Deviation: 3.110588e-02, L1: 5.089562e-05, L2: 2.809029e-05, L3: 2.660296e-04\n",
      "Stage: 0700, Loss: 2.382713e-04, Maximum_Deviation: 2.882016e-02, L1: 4.328099e-05, L2: 1.889580e-05, L3: 1.760945e-04\n",
      "Stage: 0800, Loss: 2.924510e-04, Maximum_Deviation: 2.923810e-02, L1: 3.525169e-05, L2: 2.624962e-05, L3: 2.309497e-04\n",
      "Stage: 0900, Loss: 2.194155e-04, Maximum_Deviation: 2.672529e-02, L1: 2.865789e-05, L2: 2.064014e-05, L3: 1.701175e-04\n",
      "Stage: 1000, Loss: 2.605228e-04, Maximum_Deviation: 2.148044e-02, L1: 2.552762e-05, L2: 1.901160e-05, L3: 2.159836e-04\n",
      "Stage: 1100, Loss: 2.542845e-04, Maximum_Deviation: 2.670455e-02, L1: 1.853633e-05, L2: 1.808383e-05, L3: 2.176643e-04\n",
      "Stage: 1200, Loss: 1.686312e-04, Maximum_Deviation: 2.031195e-02, L1: 2.016152e-05, L2: 2.162452e-05, L3: 1.268452e-04\n",
      "Stage: 1300, Loss: 2.595194e-04, Maximum_Deviation: 2.046311e-02, L1: 1.654615e-05, L2: 1.672132e-05, L3: 2.262519e-04\n",
      "Stage: 1400, Loss: 2.189584e-04, Maximum_Deviation: 2.367806e-02, L1: 1.868303e-05, L2: 1.283622e-05, L3: 1.874391e-04\n",
      "Stage: 1500, Loss: 1.982752e-04, Maximum_Deviation: 1.880229e-02, L1: 2.566015e-05, L2: 1.787194e-05, L3: 1.547431e-04\n",
      "Stage: 1600, Loss: 1.691838e-04, Maximum_Deviation: 1.524329e-02, L1: 1.759542e-05, L2: 1.180681e-05, L3: 1.397816e-04\n",
      "Stage: 1700, Loss: 1.434361e-04, Maximum_Deviation: 1.856065e-02, L1: 1.927779e-05, L2: 1.131549e-05, L3: 1.128429e-04\n",
      "Stage: 1800, Loss: 2.458122e-04, Maximum_Deviation: 1.470220e-02, L1: 1.842445e-05, L2: 5.486111e-06, L3: 2.219016e-04\n",
      "Stage: 1900, Loss: 1.452526e-04, Maximum_Deviation: 1.632339e-02, L1: 3.396707e-05, L2: 9.851057e-06, L3: 1.014345e-04\n",
      "Stage: 2000, Loss: 1.658911e-04, Maximum_Deviation: 1.055503e-02, L1: 2.877845e-05, L2: 5.534266e-06, L3: 1.315784e-04\n",
      "Stage: 2100, Loss: 1.136614e-04, Maximum_Deviation: 1.038730e-02, L1: 2.404639e-05, L2: 4.719295e-06, L3: 8.489568e-05\n",
      "Stage: 2200, Loss: 1.011577e-04, Maximum_Deviation: 1.359409e-02, L1: 2.316657e-05, L2: 7.574576e-06, L3: 7.041652e-05\n",
      "Stage: 2300, Loss: 7.015575e-05, Maximum_Deviation: 1.175928e-02, L1: 2.477096e-05, L2: 5.433803e-06, L3: 3.995099e-05\n",
      "Stage: 2400, Loss: 9.441328e-05, Maximum_Deviation: 8.965254e-03, L1: 2.326588e-05, L2: 3.276562e-06, L3: 6.787084e-05\n",
      "Stage: 2500, Loss: 8.214642e-05, Maximum_Deviation: 1.088864e-02, L1: 2.349385e-05, L2: 3.374989e-06, L3: 5.527758e-05\n",
      "Stage: 2600, Loss: 7.684521e-05, Maximum_Deviation: 8.228302e-03, L1: 2.217470e-05, L2: 3.154719e-06, L3: 5.151579e-05\n",
      "Stage: 2700, Loss: 6.571935e-05, Maximum_Deviation: 1.041579e-02, L1: 2.087379e-05, L2: 4.764775e-06, L3: 4.008078e-05\n",
      "Stage: 2800, Loss: 8.250794e-05, Maximum_Deviation: 1.109886e-02, L1: 1.954051e-05, L2: 5.823390e-06, L3: 5.714404e-05\n",
      "Stage: 2900, Loss: 7.993855e-05, Maximum_Deviation: 7.850289e-03, L1: 2.003855e-05, L2: 2.414151e-06, L3: 5.748585e-05\n",
      "Stage: 3000, Loss: 4.797879e-05, Maximum_Deviation: 9.344220e-03, L1: 1.455792e-05, L2: 3.213668e-06, L3: 3.020721e-05\n",
      "Stage: 3100, Loss: 6.244266e-05, Maximum_Deviation: 8.696795e-03, L1: 1.336973e-05, L2: 1.741818e-06, L3: 4.733112e-05\n",
      "Stage: 3200, Loss: 9.088615e-05, Maximum_Deviation: 6.238103e-03, L1: 1.407971e-05, L2: 9.869407e-07, L3: 7.581950e-05\n",
      "Stage: 3300, Loss: 4.942033e-05, Maximum_Deviation: 8.156538e-03, L1: 1.332475e-05, L2: 1.703900e-06, L3: 3.439168e-05\n",
      "Stage: 3400, Loss: 6.897355e-05, Maximum_Deviation: 7.574201e-03, L1: 1.112243e-05, L2: 1.858992e-06, L3: 5.599213e-05\n",
      "Stage: 3500, Loss: 1.175863e-04, Maximum_Deviation: 7.921696e-03, L1: 3.273457e-05, L2: 2.372011e-06, L3: 8.247972e-05\n",
      "Stage: 3600, Loss: 4.359985e-05, Maximum_Deviation: 7.040977e-03, L1: 6.627522e-06, L2: 1.273226e-06, L3: 3.569910e-05\n",
      "Stage: 3700, Loss: 3.511815e-05, Maximum_Deviation: 6.260276e-03, L1: 6.971692e-06, L2: 1.103899e-06, L3: 2.704256e-05\n",
      "Stage: 3800, Loss: 6.554906e-05, Maximum_Deviation: 6.428719e-03, L1: 1.598315e-05, L2: 1.400049e-06, L3: 4.816586e-05\n",
      "Stage: 3900, Loss: 1.443690e-04, Maximum_Deviation: 1.685262e-02, L1: 2.490995e-05, L2: 8.965308e-06, L3: 1.104938e-04\n",
      "Stage: 4000, Loss: 6.071702e-05, Maximum_Deviation: 7.741868e-03, L1: 1.753156e-05, L2: 1.578121e-06, L3: 4.160733e-05\n",
      "Stage: 4100, Loss: 3.240378e-05, Maximum_Deviation: 6.676376e-03, L1: 5.495876e-06, L2: 1.081463e-06, L3: 2.582644e-05\n",
      "Stage: 4200, Loss: 5.127666e-05, Maximum_Deviation: 3.694177e-03, L1: 5.318673e-06, L2: 9.703826e-08, L3: 4.586095e-05\n",
      "Stage: 4300, Loss: 3.053695e-05, Maximum_Deviation: 5.422711e-03, L1: 7.801325e-06, L2: 1.069974e-06, L3: 2.166565e-05\n",
      "Stage: 4400, Loss: 3.177946e-05, Maximum_Deviation: 1.100820e-02, L1: 6.106530e-06, L2: 2.116515e-06, L3: 2.355641e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 4500, Loss: 2.847106e-05, Maximum_Deviation: 6.229520e-03, L1: 6.053979e-06, L2: 9.427300e-07, L3: 2.147436e-05\n",
      "Stage: 4600, Loss: 6.681007e-05, Maximum_Deviation: 2.808571e-03, L1: 2.128779e-05, L2: 1.139224e-07, L3: 4.540836e-05\n",
      "Stage: 4700, Loss: 1.597720e-05, Maximum_Deviation: 7.127643e-03, L1: 3.884858e-06, L2: 8.703580e-07, L3: 1.122198e-05\n",
      "Stage: 4800, Loss: 4.027418e-05, Maximum_Deviation: 7.416606e-03, L1: 5.459971e-06, L2: 1.654229e-06, L3: 3.315998e-05\n",
      "Stage: 4900, Loss: 2.289127e-04, Maximum_Deviation: 1.768446e-02, L1: 6.288679e-05, L2: 2.358484e-05, L3: 1.424411e-04\n",
      "Stage: 5000, Loss: 1.484112e-05, Maximum_Deviation: 5.581021e-03, L1: 3.546233e-06, L2: 5.031354e-07, L3: 1.079176e-05\n",
      "Stage: 5100, Loss: 2.774787e-05, Maximum_Deviation: 4.356503e-03, L1: 4.135954e-06, L2: 2.899954e-07, L3: 2.332192e-05\n",
      "Stage: 5200, Loss: 2.289646e-05, Maximum_Deviation: 7.174611e-03, L1: 6.839091e-06, L2: 9.151778e-07, L3: 1.514219e-05\n",
      "Stage: 5300, Loss: 4.093346e-05, Maximum_Deviation: 3.146172e-03, L1: 9.914320e-06, L2: 5.765458e-08, L3: 3.096148e-05\n",
      "Stage: 5400, Loss: 1.650382e-05, Maximum_Deviation: 6.369412e-03, L1: 5.146976e-06, L2: 4.944652e-07, L3: 1.086238e-05\n",
      "Stage: 5500, Loss: 4.413616e-05, Maximum_Deviation: 4.188418e-03, L1: 1.529575e-05, L2: 2.256938e-07, L3: 2.861472e-05\n",
      "Stage: 5600, Loss: 5.863342e-05, Maximum_Deviation: 9.866416e-03, L1: 1.844774e-05, L2: 2.450478e-06, L3: 3.773521e-05\n",
      "Stage: 5700, Loss: 2.078271e-05, Maximum_Deviation: 6.876707e-03, L1: 6.430128e-06, L2: 6.952609e-07, L3: 1.365732e-05\n",
      "Stage: 5800, Loss: 2.552555e-05, Maximum_Deviation: 5.624652e-03, L1: 7.582241e-06, L2: 2.261183e-07, L3: 1.771719e-05\n",
      "Stage: 5900, Loss: 1.591095e-05, Maximum_Deviation: 5.382299e-03, L1: 4.259884e-06, L2: 4.135502e-07, L3: 1.123751e-05\n",
      "Stage: 6000, Loss: 2.833709e-05, Maximum_Deviation: 5.512476e-03, L1: 7.818172e-06, L2: 1.746752e-07, L3: 2.034424e-05\n",
      "Stage: 6100, Loss: 1.137745e-04, Maximum_Deviation: 2.640605e-03, L1: 4.417644e-05, L2: 6.031935e-08, L3: 6.953769e-05\n",
      "Stage: 6200, Loss: 2.445921e-05, Maximum_Deviation: 7.752299e-03, L1: 6.889883e-06, L2: 9.652351e-07, L3: 1.660410e-05\n",
      "Stage: 6300, Loss: 3.732563e-05, Maximum_Deviation: 8.587360e-03, L1: 5.668570e-06, L2: 4.098452e-06, L3: 2.755861e-05\n",
      "Stage: 6400, Loss: 1.680751e-05, Maximum_Deviation: 4.488111e-03, L1: 4.274466e-06, L2: 4.390653e-07, L3: 1.209397e-05\n",
      "Stage: 6500, Loss: 1.381428e-05, Maximum_Deviation: 2.752900e-03, L1: 2.304422e-06, L2: 4.130901e-07, L3: 1.109677e-05\n",
      "Stage: 6600, Loss: 1.549557e-05, Maximum_Deviation: 7.839203e-03, L1: 3.339674e-06, L2: 5.292553e-07, L3: 1.162664e-05\n",
      "Stage: 6700, Loss: 1.901633e-05, Maximum_Deviation: 2.602458e-03, L1: 5.252508e-06, L2: 3.764698e-08, L3: 1.372617e-05\n",
      "Stage: 6800, Loss: 3.370732e-04, Maximum_Deviation: 1.601720e-02, L1: 2.328647e-04, L2: 7.851520e-06, L3: 9.635693e-05\n",
      "Stage: 6900, Loss: 1.203376e-05, Maximum_Deviation: 6.074071e-03, L1: 4.189790e-06, L2: 2.236006e-07, L3: 7.620366e-06\n",
      "Stage: 7000, Loss: 1.660599e-05, Maximum_Deviation: 6.668210e-03, L1: 1.924345e-06, L2: 7.797317e-07, L3: 1.390191e-05\n",
      "Stage: 7100, Loss: 1.512919e-05, Maximum_Deviation: 4.850984e-03, L1: 1.718528e-06, L2: 3.433427e-07, L3: 1.306732e-05\n",
      "Stage: 7200, Loss: 1.928086e-05, Maximum_Deviation: 5.352736e-03, L1: 2.195724e-06, L2: 8.983359e-08, L3: 1.699530e-05\n",
      "Stage: 7300, Loss: 2.064144e-05, Maximum_Deviation: 1.491725e-03, L1: 4.813169e-06, L2: 1.364625e-08, L3: 1.581462e-05\n",
      "Stage: 7400, Loss: 2.220138e-05, Maximum_Deviation: 5.057335e-03, L1: 8.440102e-06, L2: 7.294769e-07, L3: 1.303180e-05\n",
      "Stage: 7500, Loss: 1.454219e-05, Maximum_Deviation: 5.269766e-03, L1: 3.780536e-06, L2: 4.548074e-07, L3: 1.030684e-05\n",
      "Stage: 7600, Loss: 3.581584e-05, Maximum_Deviation: 3.552318e-03, L1: 1.226792e-05, L2: 5.343113e-08, L3: 2.349448e-05\n",
      "Stage: 7700, Loss: 1.319505e-05, Maximum_Deviation: 5.614400e-03, L1: 2.159890e-06, L2: 9.870627e-07, L3: 1.004810e-05\n",
      "Stage: 7800, Loss: 2.205385e-05, Maximum_Deviation: 4.221082e-03, L1: 1.590510e-06, L2: 6.979665e-08, L3: 2.039355e-05\n",
      "Stage: 7900, Loss: 1.207639e-04, Maximum_Deviation: 3.508568e-03, L1: 2.897827e-05, L2: 3.410953e-07, L3: 9.144456e-05\n",
      "Stage: 7999, Loss: 9.799414e-06, Maximum_Deviation: 3.550172e-03, L1: 2.373003e-06, L2: 6.451369e-08, L3: 7.361897e-06\n",
      "3\n",
      "This is the fourth run of the neural network\n",
      "Stage: 0000, Loss: 7.525573e-01, Maximum_Deviation: 9.468725e-01, L1: 3.980344e-02, L2: 4.228415e-01, L3: 2.899123e-01\n",
      "Stage: 0100, Loss: 9.469801e-04, Maximum_Deviation: 2.306122e-02, L1: 1.257149e-04, L2: 4.198216e-05, L3: 7.792831e-04\n",
      "Stage: 0200, Loss: 6.750749e-04, Maximum_Deviation: 2.280253e-02, L1: 1.272900e-04, L2: 4.573828e-05, L3: 5.020467e-04\n",
      "Stage: 0300, Loss: 4.724142e-04, Maximum_Deviation: 1.849437e-02, L1: 1.536706e-04, L2: 4.266355e-05, L3: 2.760801e-04\n",
      "Stage: 0400, Loss: 3.824713e-04, Maximum_Deviation: 2.526361e-02, L1: 1.357512e-04, L2: 3.353179e-05, L3: 2.131883e-04\n",
      "Stage: 0500, Loss: 5.379122e-04, Maximum_Deviation: 2.339524e-02, L1: 9.191922e-05, L2: 2.364704e-05, L3: 4.223460e-04\n",
      "Stage: 0600, Loss: 3.607518e-04, Maximum_Deviation: 3.701973e-02, L1: 8.778049e-05, L2: 3.403040e-05, L3: 2.389409e-04\n",
      "Stage: 0700, Loss: 2.327893e-04, Maximum_Deviation: 1.687092e-02, L1: 5.997040e-05, L2: 1.844986e-05, L3: 1.543690e-04\n",
      "Stage: 0800, Loss: 2.667356e-04, Maximum_Deviation: 1.222754e-02, L1: 3.974730e-05, L2: 1.127132e-05, L3: 2.157170e-04\n",
      "Stage: 0900, Loss: 1.715668e-04, Maximum_Deviation: 1.474112e-02, L1: 2.487180e-05, L2: 1.191579e-05, L3: 1.347792e-04\n",
      "Stage: 1000, Loss: 1.544600e-04, Maximum_Deviation: 1.767474e-02, L1: 2.073972e-05, L2: 1.024867e-05, L3: 1.234716e-04\n",
      "Stage: 1100, Loss: 1.891227e-04, Maximum_Deviation: 1.954448e-02, L1: 1.808227e-05, L2: 1.343942e-05, L3: 1.576010e-04\n",
      "Stage: 1200, Loss: 1.078373e-04, Maximum_Deviation: 1.760703e-02, L1: 1.625285e-05, L2: 9.701931e-06, L3: 8.188251e-05\n",
      "Stage: 1300, Loss: 1.083324e-04, Maximum_Deviation: 1.554942e-02, L1: 1.740838e-05, L2: 7.727167e-06, L3: 8.319681e-05\n",
      "Stage: 1400, Loss: 1.163939e-04, Maximum_Deviation: 1.554525e-02, L1: 1.515879e-05, L2: 4.789803e-06, L3: 9.644533e-05\n",
      "Stage: 1500, Loss: 1.080948e-04, Maximum_Deviation: 1.036590e-02, L1: 1.562019e-05, L2: 3.536285e-06, L3: 8.893834e-05\n",
      "Stage: 1600, Loss: 6.812837e-05, Maximum_Deviation: 1.001370e-02, L1: 1.572157e-05, L2: 3.324876e-06, L3: 4.908193e-05\n",
      "Stage: 1700, Loss: 6.726172e-05, Maximum_Deviation: 6.248653e-03, L1: 1.401026e-05, L2: 2.023026e-06, L3: 5.122843e-05\n",
      "Stage: 1800, Loss: 4.865887e-05, Maximum_Deviation: 6.370127e-03, L1: 1.357810e-05, L2: 1.692224e-06, L3: 3.338854e-05\n",
      "Stage: 1900, Loss: 4.320319e-05, Maximum_Deviation: 8.300304e-03, L1: 1.225813e-05, L2: 2.126974e-06, L3: 2.881808e-05\n",
      "Stage: 2000, Loss: 3.928342e-05, Maximum_Deviation: 4.401207e-03, L1: 9.676674e-06, L2: 3.993651e-07, L3: 2.920737e-05\n",
      "Stage: 2100, Loss: 3.065216e-05, Maximum_Deviation: 4.125059e-03, L1: 8.302170e-06, L2: 4.934350e-07, L3: 2.185655e-05\n",
      "Stage: 2200, Loss: 3.566769e-05, Maximum_Deviation: 7.466257e-03, L1: 1.227105e-05, L2: 1.884883e-06, L3: 2.151175e-05\n",
      "Stage: 2300, Loss: 4.126085e-05, Maximum_Deviation: 4.313171e-03, L1: 9.806115e-06, L2: 1.994890e-07, L3: 3.125524e-05\n",
      "Stage: 2400, Loss: 2.175478e-05, Maximum_Deviation: 5.545735e-03, L1: 8.470343e-06, L2: 1.241916e-06, L3: 1.204252e-05\n",
      "Stage: 2500, Loss: 1.324382e-05, Maximum_Deviation: 4.908442e-03, L1: 4.744854e-06, L2: 5.247422e-07, L3: 7.974226e-06\n",
      "Stage: 2600, Loss: 1.868877e-05, Maximum_Deviation: 5.644143e-03, L1: 6.948680e-06, L2: 4.909201e-07, L3: 1.124917e-05\n",
      "Stage: 2700, Loss: 3.193564e-05, Maximum_Deviation: 7.297933e-03, L1: 7.042507e-06, L2: 1.307734e-06, L3: 2.358540e-05\n",
      "Stage: 2800, Loss: 1.670677e-05, Maximum_Deviation: 5.661547e-03, L1: 4.377524e-06, L2: 4.442613e-07, L3: 1.188499e-05\n",
      "Stage: 2900, Loss: 1.428969e-05, Maximum_Deviation: 3.867805e-03, L1: 3.073200e-06, L2: 1.696907e-07, L3: 1.104680e-05\n",
      "Stage: 3000, Loss: 1.414146e-04, Maximum_Deviation: 1.568836e-02, L1: 5.497766e-05, L2: 1.662629e-05, L3: 6.981065e-05\n",
      "Stage: 3100, Loss: 4.182005e-05, Maximum_Deviation: 6.709695e-03, L1: 1.563236e-05, L2: 2.796480e-06, L3: 2.339120e-05\n",
      "Stage: 3200, Loss: 1.253682e-05, Maximum_Deviation: 4.806042e-03, L1: 2.841783e-06, L2: 2.391608e-07, L3: 9.455873e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 3300, Loss: 1.578425e-05, Maximum_Deviation: 5.276024e-03, L1: 3.488352e-06, L2: 8.738880e-07, L3: 1.142201e-05\n",
      "Stage: 3400, Loss: 3.290618e-05, Maximum_Deviation: 2.263367e-03, L1: 8.204531e-06, L2: 1.143410e-08, L3: 2.469021e-05\n",
      "Stage: 3500, Loss: 2.635144e-05, Maximum_Deviation: 2.069831e-03, L1: 9.065566e-06, L2: 1.211858e-08, L3: 1.727375e-05\n",
      "Stage: 3600, Loss: 8.019225e-05, Maximum_Deviation: 6.522000e-03, L1: 2.335098e-05, L2: 2.676349e-07, L3: 5.657364e-05\n",
      "Stage: 3700, Loss: 4.183829e-05, Maximum_Deviation: 6.320834e-03, L1: 1.573267e-05, L2: 1.370707e-06, L3: 2.473491e-05\n",
      "Stage: 3800, Loss: 1.739927e-05, Maximum_Deviation: 3.344417e-03, L1: 5.637025e-06, L2: 6.231563e-08, L3: 1.169993e-05\n",
      "Stage: 3900, Loss: 9.703922e-06, Maximum_Deviation: 4.715383e-03, L1: 2.849926e-06, L2: 4.385114e-07, L3: 6.415485e-06\n",
      "Stage: 4000, Loss: 1.858484e-05, Maximum_Deviation: 6.306708e-03, L1: 5.749803e-06, L2: 8.060929e-07, L3: 1.202895e-05\n",
      "Stage: 4100, Loss: 1.133530e-05, Maximum_Deviation: 6.234229e-03, L1: 2.508560e-06, L2: 5.924342e-07, L3: 8.234309e-06\n",
      "Stage: 4200, Loss: 1.561628e-04, Maximum_Deviation: 0.000000e+00, L1: 6.464107e-05, L2: 0.000000e+00, L3: 9.152172e-05\n",
      "Stage: 4300, Loss: 6.995690e-06, Maximum_Deviation: 3.920257e-03, L1: 1.517528e-06, L2: 6.829931e-08, L3: 5.409862e-06\n",
      "Stage: 4400, Loss: 1.069028e-05, Maximum_Deviation: 4.763484e-03, L1: 2.057588e-06, L2: 1.866539e-07, L3: 8.446036e-06\n",
      "Stage: 4500, Loss: 7.746961e-06, Maximum_Deviation: 3.550947e-03, L1: 2.932712e-06, L2: 1.638253e-07, L3: 4.650425e-06\n",
      "Stage: 4600, Loss: 7.965847e-06, Maximum_Deviation: 4.685462e-03, L1: 1.750749e-06, L2: 1.327444e-07, L3: 6.082354e-06\n",
      "Stage: 4700, Loss: 1.111003e-05, Maximum_Deviation: 4.181206e-03, L1: 4.712111e-06, L2: 1.511707e-07, L3: 6.246751e-06\n",
      "Stage: 4800, Loss: 3.633399e-05, Maximum_Deviation: 8.018970e-03, L1: 1.590390e-05, L2: 7.876575e-07, L3: 1.964243e-05\n",
      "Stage: 4900, Loss: 3.894681e-06, Maximum_Deviation: 2.863765e-03, L1: 1.565210e-06, L2: 4.668413e-08, L3: 2.282786e-06\n",
      "Stage: 5000, Loss: 1.542502e-04, Maximum_Deviation: 3.854096e-03, L1: 4.053827e-05, L2: 1.347812e-07, L3: 1.135771e-04\n",
      "Stage: 5100, Loss: 1.375659e-05, Maximum_Deviation: 5.040586e-03, L1: 4.082627e-06, L2: 3.549890e-07, L3: 9.318978e-06\n",
      "Stage: 5200, Loss: 1.246481e-05, Maximum_Deviation: 4.243195e-03, L1: 3.288037e-06, L2: 1.247681e-06, L3: 7.929091e-06\n",
      "Stage: 5300, Loss: 6.405026e-06, Maximum_Deviation: 2.684295e-03, L1: 2.717705e-06, L2: 1.837305e-07, L3: 3.503591e-06\n",
      "Stage: 5400, Loss: 2.791385e-05, Maximum_Deviation: 5.648553e-03, L1: 9.940467e-06, L2: 1.533950e-06, L3: 1.643944e-05\n",
      "Stage: 5500, Loss: 8.833500e-05, Maximum_Deviation: 1.107943e-02, L1: 2.730901e-05, L2: 8.351762e-06, L3: 5.267423e-05\n",
      "Stage: 5600, Loss: 6.625648e-06, Maximum_Deviation: 2.929807e-03, L1: 2.230102e-06, L2: 2.395451e-07, L3: 4.156002e-06\n",
      "Stage: 5700, Loss: 1.783274e-05, Maximum_Deviation: 4.456580e-03, L1: 6.067297e-06, L2: 2.005801e-07, L3: 1.156486e-05\n",
      "Stage: 5800, Loss: 1.697806e-05, Maximum_Deviation: 4.197657e-03, L1: 6.333635e-06, L2: 3.659225e-07, L3: 1.027850e-05\n",
      "Stage: 5900, Loss: 7.860731e-06, Maximum_Deviation: 2.531171e-03, L1: 1.235419e-06, L2: 3.367520e-08, L3: 6.591637e-06\n",
      "Stage: 6000, Loss: 1.350909e-05, Maximum_Deviation: 5.481780e-03, L1: 4.449588e-06, L2: 8.933838e-07, L3: 8.166119e-06\n",
      "Stage: 6100, Loss: 6.798442e-06, Maximum_Deviation: 3.893256e-03, L1: 1.335093e-06, L2: 6.413144e-08, L3: 5.399218e-06\n",
      "Stage: 6200, Loss: 2.280143e-05, Maximum_Deviation: 2.763927e-03, L1: 1.395198e-05, L2: 5.477366e-07, L3: 8.301720e-06\n",
      "Stage: 6300, Loss: 2.580947e-05, Maximum_Deviation: 3.052115e-03, L1: 8.764166e-06, L2: 3.520115e-08, L3: 1.701010e-05\n",
      "Stage: 6400, Loss: 1.085113e-05, Maximum_Deviation: 2.155125e-03, L1: 2.021681e-06, L2: 2.211825e-08, L3: 8.807330e-06\n",
      "Stage: 6500, Loss: 9.674160e-06, Maximum_Deviation: 3.628254e-03, L1: 2.287585e-06, L2: 1.303875e-07, L3: 7.256186e-06\n",
      "Stage: 6600, Loss: 6.824157e-06, Maximum_Deviation: 3.544331e-03, L1: 1.911364e-06, L2: 7.134757e-08, L3: 4.841445e-06\n",
      "Stage: 6700, Loss: 1.804040e-05, Maximum_Deviation: 2.404034e-03, L1: 7.999135e-06, L2: 2.492594e-08, L3: 1.001634e-05\n",
      "Stage: 6800, Loss: 7.463171e-06, Maximum_Deviation: 3.434181e-03, L1: 3.340199e-06, L2: 3.888410e-08, L3: 4.084088e-06\n",
      "Stage: 6900, Loss: 9.831791e-06, Maximum_Deviation: 5.564153e-03, L1: 2.354935e-06, L2: 4.580403e-07, L3: 7.018815e-06\n",
      "Stage: 7000, Loss: 1.478234e-05, Maximum_Deviation: 3.853142e-03, L1: 6.433161e-06, L2: 6.568592e-07, L3: 7.692316e-06\n",
      "Stage: 7100, Loss: 3.924540e-06, Maximum_Deviation: 3.130913e-03, L1: 2.950794e-06, L2: 3.619072e-08, L3: 9.375549e-07\n",
      "Stage: 7200, Loss: 5.132319e-05, Maximum_Deviation: 8.887053e-03, L1: 1.749203e-05, L2: 2.843605e-06, L3: 3.098755e-05\n",
      "Stage: 7300, Loss: 2.144084e-05, Maximum_Deviation: 6.739259e-03, L1: 7.970120e-06, L2: 8.665253e-07, L3: 1.260420e-05\n",
      "Stage: 7400, Loss: 3.207647e-05, Maximum_Deviation: 2.604961e-03, L1: 1.088685e-05, L2: 7.962708e-09, L3: 2.118165e-05\n",
      "Stage: 7500, Loss: 6.611710e-06, Maximum_Deviation: 2.390563e-03, L1: 2.140529e-06, L2: 3.490026e-08, L3: 4.436280e-06\n",
      "Stage: 7600, Loss: 9.574029e-06, Maximum_Deviation: 3.717601e-03, L1: 1.952267e-06, L2: 9.585970e-08, L3: 7.525902e-06\n",
      "Stage: 7700, Loss: 1.615584e-05, Maximum_Deviation: 5.516589e-03, L1: 2.881816e-06, L2: 2.577384e-06, L3: 1.069664e-05\n",
      "Stage: 7800, Loss: 2.932454e-05, Maximum_Deviation: 7.682443e-04, L1: 8.489094e-06, L2: 9.982470e-10, L3: 2.083444e-05\n",
      "Stage: 7900, Loss: 8.488247e-06, Maximum_Deviation: 3.688395e-03, L1: 2.726060e-06, L2: 1.239601e-07, L3: 5.638227e-06\n",
      "Stage: 7999, Loss: 6.908803e-06, Maximum_Deviation: 3.455579e-03, L1: 2.492859e-06, L2: 5.314741e-08, L3: 4.362797e-06\n",
      "4\n",
      "This is the fifth run of the neural network\n",
      "Stage: 0000, Loss: 1.124414e+00, Maximum_Deviation: 1.541707e+00, L1: 4.721973e-02, L2: 5.062943e-01, L3: 5.709001e-01\n",
      "Stage: 0100, Loss: 8.109090e-04, Maximum_Deviation: 2.913693e-02, L1: 1.879747e-04, L2: 3.568899e-05, L3: 5.872453e-04\n",
      "Stage: 0200, Loss: 6.709437e-04, Maximum_Deviation: 2.347341e-02, L1: 1.376136e-04, L2: 3.922602e-05, L3: 4.941041e-04\n",
      "Stage: 0300, Loss: 3.269270e-04, Maximum_Deviation: 1.580635e-02, L1: 1.194577e-04, L2: 1.938209e-05, L3: 1.880872e-04\n",
      "Stage: 0400, Loss: 3.461000e-04, Maximum_Deviation: 1.867831e-02, L1: 8.951966e-05, L2: 2.720696e-05, L3: 2.293734e-04\n",
      "Stage: 0500, Loss: 2.763487e-04, Maximum_Deviation: 1.567203e-02, L1: 5.582152e-05, L2: 2.252603e-05, L3: 1.980011e-04\n",
      "Stage: 0600, Loss: 2.471663e-04, Maximum_Deviation: 1.868969e-02, L1: 4.957394e-05, L2: 2.662846e-05, L3: 1.709639e-04\n",
      "Stage: 0700, Loss: 4.315265e-04, Maximum_Deviation: 1.367342e-02, L1: 2.421547e-05, L2: 1.294007e-05, L3: 3.943710e-04\n",
      "Stage: 0800, Loss: 2.011940e-04, Maximum_Deviation: 1.460472e-02, L1: 1.622384e-05, L2: 1.327733e-05, L3: 1.716929e-04\n",
      "Stage: 0900, Loss: 1.811016e-04, Maximum_Deviation: 1.572114e-02, L1: 1.424913e-05, L2: 1.058344e-05, L3: 1.562691e-04\n",
      "Stage: 1000, Loss: 2.445194e-04, Maximum_Deviation: 1.667365e-02, L1: 1.373522e-05, L2: 1.654563e-05, L3: 2.142386e-04\n",
      "Stage: 1100, Loss: 1.655911e-04, Maximum_Deviation: 1.826093e-02, L1: 1.574549e-05, L2: 1.809658e-05, L3: 1.317490e-04\n",
      "Stage: 1200, Loss: 1.723196e-04, Maximum_Deviation: 1.577461e-02, L1: 1.257226e-05, L2: 1.278695e-05, L3: 1.469604e-04\n",
      "Stage: 1300, Loss: 8.681064e-05, Maximum_Deviation: 1.298606e-02, L1: 1.640008e-05, L2: 5.882968e-06, L3: 6.452759e-05\n",
      "Stage: 1400, Loss: 1.529820e-04, Maximum_Deviation: 1.245713e-02, L1: 1.639651e-05, L2: 9.999870e-06, L3: 1.265856e-04\n",
      "Stage: 1500, Loss: 9.033651e-05, Maximum_Deviation: 1.287919e-02, L1: 1.434030e-05, L2: 9.029001e-06, L3: 6.696722e-05\n",
      "Stage: 1600, Loss: 1.199325e-04, Maximum_Deviation: 9.636700e-03, L1: 1.985396e-05, L2: 6.322302e-06, L3: 9.375620e-05\n",
      "Stage: 1700, Loss: 9.138789e-05, Maximum_Deviation: 7.918358e-03, L1: 1.551003e-05, L2: 2.302074e-06, L3: 7.357579e-05\n",
      "Stage: 1800, Loss: 8.977085e-05, Maximum_Deviation: 8.064061e-03, L1: 1.388504e-05, L2: 3.221160e-06, L3: 7.266465e-05\n",
      "Stage: 1900, Loss: 7.168501e-05, Maximum_Deviation: 1.034945e-02, L1: 2.350013e-05, L2: 4.735585e-06, L3: 4.344929e-05\n",
      "Stage: 2000, Loss: 6.960276e-05, Maximum_Deviation: 1.013967e-02, L1: 1.507728e-05, L2: 3.263101e-06, L3: 5.126239e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 2100, Loss: 6.473569e-05, Maximum_Deviation: 9.691060e-03, L1: 9.829463e-06, L2: 1.902590e-06, L3: 5.300363e-05\n",
      "Stage: 2200, Loss: 5.210121e-05, Maximum_Deviation: 1.109827e-02, L1: 9.820498e-06, L2: 4.552051e-06, L3: 3.772866e-05\n",
      "Stage: 2300, Loss: 7.452020e-05, Maximum_Deviation: 9.430766e-03, L1: 1.306099e-05, L2: 3.107676e-06, L3: 5.835153e-05\n",
      "Stage: 2400, Loss: 3.322871e-05, Maximum_Deviation: 7.928640e-03, L1: 8.785191e-06, L2: 1.953735e-06, L3: 2.248979e-05\n",
      "Stage: 2500, Loss: 4.418209e-05, Maximum_Deviation: 6.710052e-03, L1: 6.643264e-06, L2: 1.135305e-06, L3: 3.640352e-05\n",
      "Stage: 2600, Loss: 3.600227e-05, Maximum_Deviation: 8.470297e-03, L1: 5.861550e-06, L2: 2.152028e-06, L3: 2.798869e-05\n",
      "Stage: 2700, Loss: 3.697769e-05, Maximum_Deviation: 9.095758e-03, L1: 8.813716e-06, L2: 3.519276e-06, L3: 2.464469e-05\n",
      "Stage: 2800, Loss: 2.488460e-05, Maximum_Deviation: 7.534057e-03, L1: 7.077152e-06, L2: 1.822363e-06, L3: 1.598508e-05\n",
      "Stage: 2900, Loss: 4.613861e-05, Maximum_Deviation: 5.018950e-03, L1: 7.310464e-06, L2: 4.649339e-07, L3: 3.836321e-05\n",
      "Stage: 3000, Loss: 3.929771e-05, Maximum_Deviation: 7.266939e-03, L1: 6.840065e-06, L2: 1.215657e-06, L3: 3.124198e-05\n",
      "Stage: 3100, Loss: 3.812482e-05, Maximum_Deviation: 7.208645e-03, L1: 7.268622e-06, L2: 1.800980e-06, L3: 2.905522e-05\n",
      "Stage: 3200, Loss: 3.009425e-05, Maximum_Deviation: 9.061128e-03, L1: 6.352179e-06, L2: 3.569794e-06, L3: 2.017228e-05\n",
      "Stage: 3300, Loss: 4.195566e-05, Maximum_Deviation: 5.680859e-03, L1: 4.241451e-06, L2: 6.611047e-07, L3: 3.705311e-05\n",
      "Stage: 3400, Loss: 1.770169e-05, Maximum_Deviation: 7.557124e-03, L1: 3.552550e-06, L2: 1.674042e-06, L3: 1.247510e-05\n",
      "Stage: 3500, Loss: 2.393184e-05, Maximum_Deviation: 3.144354e-03, L1: 5.231968e-06, L2: 1.285303e-07, L3: 1.857134e-05\n",
      "Stage: 3600, Loss: 4.109377e-05, Maximum_Deviation: 4.711270e-03, L1: 9.632813e-06, L2: 6.893131e-07, L3: 3.077164e-05\n",
      "Stage: 3700, Loss: 3.025601e-05, Maximum_Deviation: 3.744811e-03, L1: 5.154585e-06, L2: 1.333907e-07, L3: 2.496804e-05\n",
      "Stage: 3800, Loss: 1.917741e-05, Maximum_Deviation: 6.591797e-03, L1: 6.023814e-06, L2: 1.049395e-06, L3: 1.210420e-05\n",
      "Stage: 3900, Loss: 3.297045e-05, Maximum_Deviation: 9.063154e-03, L1: 6.454839e-06, L2: 3.007762e-06, L3: 2.350785e-05\n",
      "Stage: 4000, Loss: 4.847084e-05, Maximum_Deviation: 1.141512e-02, L1: 5.885606e-06, L2: 4.592602e-06, L3: 3.799263e-05\n",
      "Stage: 4100, Loss: 2.727686e-05, Maximum_Deviation: 5.112797e-03, L1: 3.610139e-06, L2: 2.794196e-07, L3: 2.338730e-05\n",
      "Stage: 4200, Loss: 2.924461e-05, Maximum_Deviation: 7.535011e-03, L1: 6.384263e-06, L2: 1.331421e-06, L3: 2.152892e-05\n",
      "Stage: 4300, Loss: 1.883493e-05, Maximum_Deviation: 6.212503e-03, L1: 3.895500e-06, L2: 7.281084e-07, L3: 1.421133e-05\n",
      "Stage: 4400, Loss: 1.952509e-05, Maximum_Deviation: 4.233927e-03, L1: 4.476350e-06, L2: 1.708404e-07, L3: 1.487789e-05\n",
      "Stage: 4500, Loss: 1.758999e-05, Maximum_Deviation: 7.226646e-03, L1: 4.464695e-06, L2: 8.974308e-07, L3: 1.222787e-05\n",
      "Stage: 4600, Loss: 2.037022e-05, Maximum_Deviation: 7.550985e-03, L1: 7.619072e-06, L2: 1.008295e-06, L3: 1.174285e-05\n",
      "Stage: 4700, Loss: 1.815859e-05, Maximum_Deviation: 7.566124e-03, L1: 3.177089e-06, L2: 1.154912e-06, L3: 1.382659e-05\n",
      "Stage: 4800, Loss: 1.500294e-05, Maximum_Deviation: 3.163159e-03, L1: 3.143253e-06, L2: 1.410748e-08, L3: 1.184558e-05\n",
      "Stage: 4900, Loss: 2.513101e-05, Maximum_Deviation: 4.972935e-03, L1: 6.002451e-06, L2: 7.009523e-07, L3: 1.842761e-05\n",
      "Stage: 5000, Loss: 2.702996e-05, Maximum_Deviation: 2.852261e-03, L1: 5.287704e-06, L2: 3.954502e-08, L3: 2.170271e-05\n",
      "Stage: 5100, Loss: 2.366314e-05, Maximum_Deviation: 6.997794e-03, L1: 1.069736e-05, L2: 9.024601e-07, L3: 1.206332e-05\n",
      "Stage: 5200, Loss: 1.229201e-05, Maximum_Deviation: 5.522639e-03, L1: 4.266139e-06, L2: 1.412597e-07, L3: 7.884609e-06\n",
      "Stage: 5300, Loss: 1.422122e-05, Maximum_Deviation: 5.024076e-03, L1: 2.774881e-06, L2: 4.393327e-07, L3: 1.100701e-05\n",
      "Stage: 5400, Loss: 1.661093e-05, Maximum_Deviation: 6.355375e-03, L1: 2.896285e-06, L2: 1.903354e-06, L3: 1.181130e-05\n",
      "Stage: 5500, Loss: 2.827382e-05, Maximum_Deviation: 1.394272e-03, L1: 7.959226e-06, L2: 7.108206e-09, L3: 2.030749e-05\n",
      "Stage: 5600, Loss: 1.594524e-05, Maximum_Deviation: 5.971342e-03, L1: 2.633701e-06, L2: 6.135504e-07, L3: 1.269799e-05\n",
      "Stage: 5700, Loss: 1.546692e-05, Maximum_Deviation: 2.585679e-03, L1: 3.580263e-06, L2: 3.453636e-08, L3: 1.185212e-05\n",
      "Stage: 5800, Loss: 1.401674e-05, Maximum_Deviation: 3.612101e-03, L1: 3.039700e-06, L2: 2.396953e-07, L3: 1.073735e-05\n",
      "Stage: 5900, Loss: 1.017389e-05, Maximum_Deviation: 3.768921e-03, L1: 2.941396e-06, L2: 6.958896e-08, L3: 7.162904e-06\n",
      "Stage: 6000, Loss: 8.831435e-06, Maximum_Deviation: 5.886883e-03, L1: 2.302814e-06, L2: 5.040711e-07, L3: 6.024549e-06\n",
      "Stage: 6100, Loss: 4.903032e-06, Maximum_Deviation: 2.362579e-03, L1: 2.963244e-06, L2: 2.505285e-08, L3: 1.914735e-06\n",
      "Stage: 6200, Loss: 2.121376e-05, Maximum_Deviation: 5.327225e-03, L1: 6.527634e-06, L2: 6.459114e-07, L3: 1.404021e-05\n",
      "Stage: 6300, Loss: 1.363481e-04, Maximum_Deviation: 3.105968e-03, L1: 4.607096e-05, L2: 1.431242e-07, L3: 9.013401e-05\n",
      "Stage: 6400, Loss: 1.301170e-05, Maximum_Deviation: 2.631277e-03, L1: 2.210661e-06, L2: 1.814228e-08, L3: 1.078290e-05\n",
      "Stage: 6500, Loss: 8.679683e-05, Maximum_Deviation: 9.055495e-03, L1: 2.115756e-05, L2: 7.974828e-06, L3: 5.766445e-05\n",
      "Stage: 6600, Loss: 1.365812e-05, Maximum_Deviation: 2.783388e-03, L1: 5.708297e-06, L2: 1.939635e-07, L3: 7.755864e-06\n",
      "Stage: 6700, Loss: 2.112824e-05, Maximum_Deviation: 7.409036e-03, L1: 4.768157e-06, L2: 8.940207e-07, L3: 1.546606e-05\n",
      "Stage: 6800, Loss: 2.567394e-05, Maximum_Deviation: 6.738514e-03, L1: 8.088810e-06, L2: 1.403796e-06, L3: 1.618133e-05\n",
      "Stage: 6900, Loss: 6.241521e-05, Maximum_Deviation: 9.096622e-03, L1: 2.474038e-05, L2: 2.230196e-06, L3: 3.544463e-05\n",
      "Stage: 7000, Loss: 1.884527e-05, Maximum_Deviation: 9.383559e-04, L1: 3.616011e-06, L2: 1.076891e-09, L3: 1.522818e-05\n",
      "Stage: 7100, Loss: 9.075731e-06, Maximum_Deviation: 3.123254e-03, L1: 1.884599e-06, L2: 3.153338e-08, L3: 7.159599e-06\n",
      "Stage: 7200, Loss: 6.475068e-06, Maximum_Deviation: 3.371358e-03, L1: 1.889951e-06, L2: 4.219264e-08, L3: 4.542925e-06\n",
      "Stage: 7300, Loss: 4.407217e-05, Maximum_Deviation: 9.576797e-03, L1: 6.968406e-06, L2: 2.949943e-06, L3: 3.415383e-05\n",
      "Stage: 7400, Loss: 2.442791e-05, Maximum_Deviation: 5.087912e-03, L1: 9.322294e-06, L2: 1.494540e-06, L3: 1.361107e-05\n",
      "Stage: 7500, Loss: 8.925163e-06, Maximum_Deviation: 1.036137e-03, L1: 2.684663e-06, L2: 1.881573e-09, L3: 6.238619e-06\n",
      "Stage: 7600, Loss: 7.525083e-06, Maximum_Deviation: 3.762394e-03, L1: 2.474077e-06, L2: 8.481251e-08, L3: 4.966194e-06\n",
      "Stage: 7700, Loss: 1.240135e-04, Maximum_Deviation: 1.833463e-02, L1: 1.743263e-05, L2: 7.885049e-06, L3: 9.869585e-05\n",
      "Stage: 7800, Loss: 1.059006e-05, Maximum_Deviation: 0.000000e+00, L1: 1.593160e-06, L2: 0.000000e+00, L3: 8.996900e-06\n",
      "Stage: 7900, Loss: 1.122816e-05, Maximum_Deviation: 4.624397e-03, L1: 3.631424e-06, L2: 7.906644e-07, L3: 6.806072e-06\n",
      "Stage: 7999, Loss: 1.124397e-05, Maximum_Deviation: 6.032258e-03, L1: 2.001446e-06, L2: 2.797840e-07, L3: 8.962744e-06\n"
     ]
    }
   ],
   "source": [
    "# before opening a tensorflow session, close the old one if possible\n",
    "n=0\n",
    "while n<5:\n",
    "    try:\n",
    "        sess.close()\n",
    "    except NameError:\n",
    "        pass \n",
    "    sess =  tf.compat.v1.Session()\n",
    "###################################################L2 NORM is multiplied by 1#######################################\n",
    "    sess.run(init_op)\n",
    "\n",
    "    print(n)\n",
    "    if n+1==1:print(\"This is the first run of the neural network\")\n",
    "    elif n+1==2:print(\"This is the second run of the neural network\")\n",
    "    elif n+1==3:print(\"This is the third run of the neural network\")\n",
    "    elif n+1==4:print(\"This is the fourth run of the neural network\")\n",
    "    elif n+1==5:print(\"This is the fifth run of the neural network\")\n",
    "    for epoch in range(nr_epochs):\n",
    "        t_interior_mc, x_interior_mc, t_initial_mc, x_initial_mc = get_monte_carlo_points(N_interior, N_initial)\n",
    "\n",
    "        for _ in range(steps_per_sample):\n",
    "             Max_deviation,residual_value, residual_interior_value,residual_exterior_value,residual_initial_value, _ = \\\n",
    "                 sess.run([Max_dev,residual, residual_interior, residual_exterior,residual_initial, optimizer],\n",
    "                          feed_dict = {t_interior_tf:t_interior_mc, x_interior_tf:x_interior_mc,\n",
    "                                        t_initial_tf:t_initial_mc, x_initial_tf:x_initial_mc})\n",
    "            \n",
    "\n",
    "        if n==0:residuals_list_residual0_1.append(residual_value),residuals_list_MaxDev0_1.append(Max_deviation),residuals_list_L10_1.append(residual_interior_value),residuals_list_L20_1.append(residual_exterior_value),residuals_list_L30_1.append(residual_initial_value)\n",
    "        elif n==1:residuals_list_residual1_1.append(residual_value),residuals_list_MaxDev1_1.append(Max_deviation),residuals_list_L11_1.append(residual_interior_value),residuals_list_L21_1.append(residual_exterior_value),residuals_list_L31_1.append(residual_initial_value)\n",
    "        elif n==2:residuals_list_residual2_1.append(residual_value),residuals_list_MaxDev2_1.append(Max_deviation),residuals_list_L12_1.append(residual_interior_value),residuals_list_L22_1.append(residual_exterior_value),residuals_list_L32_1.append(residual_initial_value)\n",
    "        elif n==3:residuals_list_residual3_1.append(residual_value),residuals_list_MaxDev3_1.append(Max_deviation),residuals_list_L13_1.append(residual_interior_value),residuals_list_L23_1.append(residual_exterior_value),residuals_list_L33_1.append(residual_initial_value)\n",
    "        elif n==4:residuals_list_residual4_1.append(residual_value),residuals_list_MaxDev4_1.append(Max_deviation),residuals_list_L14_1.append(residual_interior_value),residuals_list_L24_1.append(residual_exterior_value),residuals_list_L34_1.append(residual_initial_value)\n",
    "\n",
    "        if (not np.mod(epoch, 100)) or epoch+1==nr_epochs:\n",
    "            print(\"Stage: {:04d}, Loss: {:e}, Maximum_Deviation: {:e}, L1: {:e}, L2: {:e}, L3: {:e}\".format(\n",
    "                epoch,residual_value,Max_deviation, residual_interior_value,residual_exterior_value,residual_initial_value) )\n",
    "    n=n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "N = 41      # Points on plot grid\n",
    "\n",
    "times_to_plot = [0*T_max, 0.33*T_max, 0.66*T_max, T_max]\n",
    "tplot = np.linspace(T_min, T_max, N) # for surface plot\n",
    "xplot = np.linspace(S_min, S_max, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015301346728869625\n",
      "0.0014952019578190444\n",
      "[array([[0.00223151],\n",
      "       [0.00209045],\n",
      "       [0.0019823 ],\n",
      "       [0.00190312],\n",
      "       [0.00184086],\n",
      "       [0.00177777],\n",
      "       [0.00169611],\n",
      "       [0.00158691],\n",
      "       [0.00146252],\n",
      "       [0.00136882],\n",
      "       [0.00139913],\n",
      "       [0.0017069 ],\n",
      "       [0.00251171],\n",
      "       [0.00409898],\n",
      "       [0.00680572],\n",
      "       [0.01099032],\n",
      "       [0.01698852],\n",
      "       [0.02506277],\n",
      "       [0.03536007],\n",
      "       [0.04789311],\n",
      "       [0.0625506 ],\n",
      "       [0.07913125],\n",
      "       [0.09738508],\n",
      "       [0.11705181],\n",
      "       [0.13788468],\n",
      "       [0.15966311],\n",
      "       [0.18219736],\n",
      "       [0.20532754],\n",
      "       [0.22892138],\n",
      "       [0.25287095],\n",
      "       [0.27708986],\n",
      "       [0.30151078],\n",
      "       [0.3260819 ],\n",
      "       [0.35076544],\n",
      "       [0.37553397],\n",
      "       [0.40036902],\n",
      "       [0.42525896],\n",
      "       [0.4501967 ],\n",
      "       [0.47517982],\n",
      "       [0.50020766],\n",
      "       [0.5252814 ]], dtype=float32), array([[0.00225192],\n",
      "       [0.0020934 ],\n",
      "       [0.00197592],\n",
      "       [0.00190032],\n",
      "       [0.00185737],\n",
      "       [0.00182837],\n",
      "       [0.00178993],\n",
      "       [0.0017224 ],\n",
      "       [0.00162104],\n",
      "       [0.00151113],\n",
      "       [0.00146213],\n",
      "       [0.00160357],\n",
      "       [0.0021368 ],\n",
      "       [0.00333861],\n",
      "       [0.00555351],\n",
      "       [0.00916818],\n",
      "       [0.01456669],\n",
      "       [0.02207109],\n",
      "       [0.03188545],\n",
      "       [0.04406115],\n",
      "       [0.05849907],\n",
      "       [0.07498562],\n",
      "       [0.093245  ],\n",
      "       [0.11298662],\n",
      "       [0.1339376 ],\n",
      "       [0.15585804],\n",
      "       [0.17854443],\n",
      "       [0.20182809],\n",
      "       [0.22557005],\n",
      "       [0.24965802],\n",
      "       [0.27400228],\n",
      "       [0.29853305],\n",
      "       [0.32319734],\n",
      "       [0.3479562 ],\n",
      "       [0.37278232],\n",
      "       [0.39765766],\n",
      "       [0.4225711 ],\n",
      "       [0.4475172 ],\n",
      "       [0.4724935 ],\n",
      "       [0.49750087],\n",
      "       [0.5225414 ]], dtype=float32), array([[0.00229159],\n",
      "       [0.00211856],\n",
      "       [0.00199232],\n",
      "       [0.00191921],\n",
      "       [0.00189346],\n",
      "       [0.00189701],\n",
      "       [0.00190201],\n",
      "       [0.00187966],\n",
      "       [0.0018096 ],\n",
      "       [0.00169599],\n",
      "       [0.00158387],\n",
      "       [0.00157681],\n",
      "       [0.00185248],\n",
      "       [0.00267369],\n",
      "       [0.0043861 ],\n",
      "       [0.00739977],\n",
      "       [0.01214737],\n",
      "       [0.01901862],\n",
      "       [0.02828828],\n",
      "       [0.04006073],\n",
      "       [0.05425808],\n",
      "       [0.07065463],\n",
      "       [0.08894086],\n",
      "       [0.10878718],\n",
      "       [0.12988713],\n",
      "       [0.15197715],\n",
      "       [0.17483893],\n",
      "       [0.19829479],\n",
      "       [0.22220036],\n",
      "       [0.24643984],\n",
      "       [0.27092108],\n",
      "       [0.2955719 ],\n",
      "       [0.3203384 ],\n",
      "       [0.3451809 ],\n",
      "       [0.3700725 ],\n",
      "       [0.39499536],\n",
      "       [0.41993943],\n",
      "       [0.44490018],\n",
      "       [0.46987668],\n",
      "       [0.49487075],\n",
      "       [0.5198852 ]], dtype=float32), array([[0.00233772],\n",
      "       [0.0021522 ],\n",
      "       [0.00201774],\n",
      "       [0.00194642],\n",
      "       [0.00193647],\n",
      "       [0.00197124],\n",
      "       [0.0020209 ],\n",
      "       [0.00204787],\n",
      "       [0.00201777],\n",
      "       [0.00191447],\n",
      "       [0.00175741],\n",
      "       [0.00162226],\n",
      "       [0.00165972],\n",
      "       [0.00211143],\n",
      "       [0.00331706],\n",
      "       [0.00570312],\n",
      "       [0.00974792],\n",
      "       [0.01591489],\n",
      "       [0.02456549],\n",
      "       [0.03587723],\n",
      "       [0.04980791],\n",
      "       [0.06612173],\n",
      "       [0.08446699],\n",
      "       [0.10446104],\n",
      "       [0.12575257],\n",
      "       [0.1480475 ],\n",
      "       [0.17111102],\n",
      "       [0.19475743],\n",
      "       [0.21883908],\n",
      "       [0.24323848],\n",
      "       [0.2678623 ],\n",
      "       [0.2926378 ],\n",
      "       [0.31751022],\n",
      "       [0.34243992],\n",
      "       [0.36740002],\n",
      "       [0.39237413],\n",
      "       [0.417353  ],\n",
      "       [0.44233367],\n",
      "       [0.46731582],\n",
      "       [0.49230334],\n",
      "       [0.51729965]], dtype=float32), array([[0.00237712],\n",
      "       [0.00217935],\n",
      "       [0.00203612],\n",
      "       [0.00196478],\n",
      "       [0.00196889],\n",
      "       [0.00203404],\n",
      "       [0.00212881],\n",
      "       [0.00220931],\n",
      "       [0.00222817],\n",
      "       [0.00214922],\n",
      "       [0.00196698],\n",
      "       [0.00172701],\n",
      "       [0.00154951],\n",
      "       [0.00164968],\n",
      "       [0.00235188],\n",
      "       [0.00408971],\n",
      "       [0.00738081],\n",
      "       [0.01276508],\n",
      "       [0.02070591],\n",
      "       [0.03148082],\n",
      "       [0.04510698],\n",
      "       [0.06134742],\n",
      "       [0.07979825],\n",
      "       [0.10000393],\n",
      "       [0.12154934],\n",
      "       [0.144099  ],\n",
      "       [0.16739795],\n",
      "       [0.19125459],\n",
      "       [0.2155213 ],\n",
      "       [0.24008277],\n",
      "       [0.26484773],\n",
      "       [0.28974405],\n",
      "       [0.31471804],\n",
      "       [0.33973107],\n",
      "       [0.3647568 ],\n",
      "       [0.38978007],\n",
      "       [0.41479352],\n",
      "       [0.4397953 ],\n",
      "       [0.46478727],\n",
      "       [0.48977384],\n",
      "       [0.51476014]], dtype=float32), array([[0.00240517],\n",
      "       [0.0021928 ],\n",
      "       [0.00203824],\n",
      "       [0.00196424],\n",
      "       [0.00197944],\n",
      "       [0.00207272],\n",
      "       [0.00221282],\n",
      "       [0.00235024],\n",
      "       [0.00242594],\n",
      "       [0.00238556],\n",
      "       [0.00219786],\n",
      "       [0.00187799],\n",
      "       [0.00151259],\n",
      "       [0.00128448],\n",
      "       [0.00149405],\n",
      "       [0.00257084],\n",
      "       [0.00506046],\n",
      "       [0.00957537],\n",
      "       [0.01669464],\n",
      "       [0.02682668],\n",
      "       [0.04008541],\n",
      "       [0.05625501],\n",
      "       [0.07487223],\n",
      "       [0.09538186],\n",
      "       [0.11727491],\n",
      "       [0.1401543 ],\n",
      "       [0.1637384 ],\n",
      "       [0.18783107],\n",
      "       [0.21229109],\n",
      "       [0.23701116],\n",
      "       [0.26190642],\n",
      "       [0.28690997],\n",
      "       [0.31197104],\n",
      "       [0.33705315],\n",
      "       [0.36213228],\n",
      "       [0.38719502],\n",
      "       [0.41223595],\n",
      "       [0.4372553 ],\n",
      "       [0.4622567 ],\n",
      "       [0.48724595],\n",
      "       [0.5122291 ]], dtype=float32), array([[0.00242969],\n",
      "       [0.0021981 ],\n",
      "       [0.00202754],\n",
      "       [0.00194654],\n",
      "       [0.00196892],\n",
      "       [0.00208771],\n",
      "       [0.00227225],\n",
      "       [0.0024693 ],\n",
      "       [0.00260913],\n",
      "       [0.00262034],\n",
      "       [0.00244698],\n",
      "       [0.00207308],\n",
      "       [0.00154927],\n",
      "       [0.00102133],\n",
      "       [0.00075758],\n",
      "       [0.00116944],\n",
      "       [0.00281546],\n",
      "       [0.00636941],\n",
      "       [0.01253039],\n",
      "       [0.02186939],\n",
      "       [0.0346505 ],\n",
      "       [0.05072314],\n",
      "       [0.06957284],\n",
      "       [0.09051242],\n",
      "       [0.11289176],\n",
      "       [0.13621753],\n",
      "       [0.16016576],\n",
      "       [0.18453643],\n",
      "       [0.20920238],\n",
      "       [0.2340739 ],\n",
      "       [0.25908074],\n",
      "       [0.2841669 ],\n",
      "       [0.30928865],\n",
      "       [0.33441392],\n",
      "       [0.3595226 ],\n",
      "       [0.3846043 ],\n",
      "       [0.40965644],\n",
      "       [0.43468168],\n",
      "       [0.45968583],\n",
      "       [0.48467633],\n",
      "       [0.5096605 ]], dtype=float32), array([[ 2.4647117e-03],\n",
      "       [ 2.2076964e-03],\n",
      "       [ 2.0160079e-03],\n",
      "       [ 1.9232333e-03],\n",
      "       [ 1.9492209e-03],\n",
      "       [ 2.0911098e-03],\n",
      "       [ 2.3195744e-03],\n",
      "       [ 2.5787055e-03],\n",
      "       [ 2.7900934e-03],\n",
      "       [ 2.8660595e-03],\n",
      "       [ 2.7270913e-03],\n",
      "       [ 2.3262799e-03],\n",
      "       [ 1.6769171e-03],\n",
      "       [ 8.8402629e-04],\n",
      "       [ 1.7517805e-04],\n",
      "       [-6.8634748e-05],\n",
      "       [ 7.0440769e-04],\n",
      "       [ 3.2083094e-03],\n",
      "       [ 8.2535744e-03],\n",
      "       [ 1.6596138e-02],\n",
      "       [ 2.8712720e-02],\n",
      "       [ 4.4596106e-02],\n",
      "       [ 6.3720942e-02],\n",
      "       [ 8.5242689e-02],\n",
      "       [ 1.0830510e-01],\n",
      "       [ 1.3225627e-01],\n",
      "       [ 1.5669745e-01],\n",
      "       [ 1.8141946e-01],\n",
      "       [ 2.0631912e-01],\n",
      "       [ 2.3133799e-01],\n",
      "       [ 2.5643304e-01],\n",
      "       [ 2.8156760e-01],\n",
      "       [ 3.0671105e-01],\n",
      "       [ 3.3184019e-01],\n",
      "       [ 3.5694155e-01],\n",
      "       [ 3.8200882e-01],\n",
      "       [ 4.0704367e-01],\n",
      "       [ 4.3205228e-01],\n",
      "       [ 4.5704308e-01],\n",
      "       [ 4.8202571e-01],\n",
      "       [ 5.0700855e-01]], dtype=float32), array([[ 2.5116205e-03],\n",
      "       [ 2.2239089e-03],\n",
      "       [ 2.0071268e-03],\n",
      "       [ 1.9003153e-03],\n",
      "       [ 1.9284189e-03],\n",
      "       [ 2.0931065e-03],\n",
      "       [ 2.3673475e-03],\n",
      "       [ 2.6934147e-03],\n",
      "       [ 2.9852986e-03],\n",
      "       [ 3.1409562e-03],\n",
      "       [ 3.0587614e-03],\n",
      "       [ 2.6611686e-03],\n",
      "       [ 1.9243360e-03],\n",
      "       [ 9.0926886e-04],\n",
      "       [-2.0268559e-04],\n",
      "       [-1.0743141e-03],\n",
      "       [-1.1808574e-03],\n",
      "       [ 2.0334125e-04],\n",
      "       [ 3.9728284e-03],\n",
      "       [ 1.1068493e-02],\n",
      "       [ 2.2236139e-02],\n",
      "       [ 3.7720531e-02],\n",
      "       [ 5.7083040e-02],\n",
      "       [ 7.9333961e-02],\n",
      "       [ 1.0333419e-01],\n",
      "       [ 1.2817433e-01],\n",
      "       [ 1.5331477e-01],\n",
      "       [ 1.7851755e-01],\n",
      "       [ 2.0371136e-01],\n",
      "       [ 2.2888801e-01],\n",
      "       [ 2.5404945e-01],\n",
      "       [ 2.7919260e-01],\n",
      "       [ 3.0430910e-01],\n",
      "       [ 3.2939062e-01],\n",
      "       [ 3.5443321e-01],\n",
      "       [ 3.7943873e-01],\n",
      "       [ 4.0441427e-01],\n",
      "       [ 4.2937055e-01],\n",
      "       [ 4.5431975e-01],\n",
      "       [ 4.7927412e-01],\n",
      "       [ 5.0424361e-01]], dtype=float32), array([[ 2.53129005e-03],\n",
      "       [ 2.21163034e-03],\n",
      "       [ 1.97029114e-03],\n",
      "       [ 1.85179710e-03],\n",
      "       [ 1.88580155e-03],\n",
      "       [ 2.07871199e-03],\n",
      "       [ 2.40588188e-03],\n",
      "       [ 2.80871987e-03],\n",
      "       [ 3.19483876e-03],\n",
      "       [ 3.44964862e-03],\n",
      "       [ 3.45087051e-03],\n",
      "       [ 3.09255719e-03],\n",
      "       [ 2.31358409e-03],\n",
      "       [ 1.13061070e-03],\n",
      "       [-3.25828791e-04],\n",
      "       [-1.77165866e-03],\n",
      "       [-2.72911787e-03],\n",
      "       [-2.49353051e-03],\n",
      "       [-1.29252672e-04],\n",
      "       [ 5.45680523e-03],\n",
      "       [ 1.53013468e-02],\n",
      "       [ 3.00162435e-02],\n",
      "       [ 4.94165123e-02],\n",
      "       [ 7.24643469e-02],\n",
      "       [ 9.76868570e-02],\n",
      "       [ 1.23776495e-01],\n",
      "       [ 1.49932414e-01],\n",
      "       [ 1.75833791e-01],\n",
      "       [ 2.01440901e-01],\n",
      "       [ 2.26818472e-01],\n",
      "       [ 2.52039164e-01],\n",
      "       [ 2.77153283e-01],\n",
      "       [ 3.02189142e-01],\n",
      "       [ 3.27161700e-01],\n",
      "       [ 3.52083057e-01],\n",
      "       [ 3.76965672e-01],\n",
      "       [ 4.01825219e-01],\n",
      "       [ 4.26678866e-01],\n",
      "       [ 4.51543599e-01],\n",
      "       [ 4.76435572e-01],\n",
      "       [ 5.01366615e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "max_error_1,New_mean_error_container_1=compute_errors()\n",
    "print(max_error_1)\n",
    "print(New_mean_error_container_1)\n",
    "print(nn_plot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
